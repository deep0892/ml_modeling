{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Networks_from_Scratch_Dropout.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd6r0ew6iXlT"
      },
      "source": [
        "Dropout implementation from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVlIK3jjDIGt",
        "outputId": "cc921ff2-f548-4359-cd5c-6b9b52d1871d"
      },
      "source": [
        "import random\n",
        "dropout_rate = 0.5\n",
        "# Example output containing 10 values\n",
        "example_output = [ 0.27 , - 1.03 , 0.67 , 0.99 , 0.05 , - 0.37 , - 2.01 , 1.13 , - 0.07 , 0.73 ]\n",
        "\n",
        "# Repeat as long as necessary\n",
        "while True :\n",
        "  # Randomly choose index and set value to 0\n",
        "  index = random.randint( 0 , len (example_output) - 1 )\n",
        "  example_output[index] = 0\n",
        "\n",
        "  # We might set an index that already is zeroed\n",
        "  # There are different ways of overcoming this problem,\n",
        "  # for simplicity we count values that are exactly 0\n",
        "  # while it's extremely rare in real model that weights\n",
        "  # are exactly 0, this is not the best method for sure\n",
        "  dropped_out = 0\n",
        "\n",
        "  for value in example_output:\n",
        "    if value == 0 :\n",
        "      dropped_out += 1\n",
        "\n",
        "  # If required number of outputs is zeroed - leave the loop\n",
        "  if dropped_out / len (example_output) >= dropout_rate:\n",
        "    break\n",
        "print (example_output)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, -1.03, 0, 0, 0.05, -0.37, -2.01, 1.13, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUDaCG32ibzm"
      },
      "source": [
        "Dropout implementation with numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQoS8LHmiEdx"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daG2HvzEsHh4",
        "outputId": "b0855c35-9e78-44d1-a9ca-4ace41f624a4"
      },
      "source": [
        "dropout_rate = 0.20\n",
        "np.random.binomial( 1 , 1 - dropout_rate, size = 5 )"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7uTQX5fsIRD",
        "outputId": "a579bae7-318b-418f-f0e2-c3fedc5af115"
      },
      "source": [
        "import numpy as np\n",
        "dropout_rate = 0.3\n",
        "example_output = np.array([ 0.27 , - 1.03 , 0.67 , 0.99 , 0.05 , - 0.37 , - 2.01 , 1.13 , - 0.07 , 0.73 ])\n",
        "example_output *= np.random.binomial( 1 , 1 - dropout_rate, example_output.shape)\n",
        "print (example_output)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.27 -0.    0.    0.    0.05 -0.37 -2.01  1.13 -0.    0.  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlStXLcCsolT"
      },
      "source": [
        "# Dropout\n",
        "class Layer_Dropout :\n",
        "  # Init\n",
        "  def __init__ ( self , rate ):\n",
        "    # Store rate, we invert it as for example for dropout\n",
        "    # of 0.1 we need success rate of 0.9\n",
        "    self.rate = 1 - rate\n",
        "\n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Save input values\n",
        "    self.inputs = inputs\n",
        "    # Generate and save scaled mask\n",
        "    self.binary_mask = np.random.binomial( 1 , self.rate, size = inputs.shape) / self.rate\n",
        "    # Apply mask to output values\n",
        "    self.output = inputs * self.binary_mask\n",
        "\n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues ):\n",
        "    # Gradient on values\n",
        "    self.dinputs = dvalues * self.binary_mask"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMX8SnaDxgjq"
      },
      "source": [
        "**Full code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swYfHG6jxl7L",
        "outputId": "f0637aa8-ca20-4179-b30b-1f8eff4da9b0"
      },
      "source": [
        "!pip install nnfs"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nnfs\n",
            "  Downloading https://files.pythonhosted.org/packages/06/8c/3003a41d5229e65da792331b060dcad8100a0a5b9760f8c2074cde864148/nnfs-0.5.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nnfs) (1.19.5)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5uv6HrKxBkk"
      },
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "nnfs.init()\n",
        "\n",
        "# Dense layer\n",
        "class Layer_Dense :\n",
        "  # Layer initialization\n",
        "  def __init__ ( self , n_inputs , n_neurons , \n",
        "                weight_regularizer_l1 = 0 , weight_regularizer_l2 = 0 ,\n",
        "                bias_regularizer_l1 = 0 , bias_regularizer_l2 = 0 ):\n",
        "    # Initialize weights and biases\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros(( 1 , n_neurons))\n",
        "\n",
        "    # Set regularization strength\n",
        "    self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "    self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "    self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "    self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "  \n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Remember input values\n",
        "    self.inputs = inputs\n",
        "\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues ):\n",
        "    # Gradients on parameters\n",
        "    self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "    self.dbiases = np.sum(dvalues, axis = 0 , keepdims = True )\n",
        "\n",
        "    # Gradients on regularization\n",
        "    # L1 on weights\n",
        "    if self.weight_regularizer_l1 > 0 :\n",
        "      dL1 = np.ones_like(self.weights)\n",
        "      dL1[self.weights < 0 ] = - 1\n",
        "      self.dweights += self.weight_regularizer_l1 * dL1\n",
        "\n",
        "    # L2 on weights\n",
        "    if self.weight_regularizer_l2 > 0 :\n",
        "      self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
        "\n",
        "    # L1 on biases\n",
        "    if self.bias_regularizer_l1 > 0 :\n",
        "      dL1 = np.ones_like(self.biases)\n",
        "      dL1[self.biases < 0 ] = - 1\n",
        "      self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "\n",
        "    # L2 on biases\n",
        "    if self.bias_regularizer_l2 > 0 :\n",
        "      self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
        "\n",
        "    # Gradient on values\n",
        "    self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "# Dropout\n",
        "class Layer_Dropout :\n",
        "  # Init\n",
        "  def __init__ ( self , rate ):\n",
        "    # Store rate, we invert it as for example for dropout\n",
        "    # of 0.1 we need success rate of 0.9\n",
        "    self.rate = 1 - rate\n",
        "\n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Save input values\n",
        "    self.inputs = inputs\n",
        "    # Generate and save scaled mask\n",
        "    self.binary_mask = np.random.binomial( 1 , self.rate, size = inputs.shape) / self.rate\n",
        "    # Apply mask to output values\n",
        "    self.output = inputs * self.binary_mask\n",
        "\n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues ):\n",
        "    # Gradient on values\n",
        "    self.dinputs = dvalues * self.binary_mask\n",
        "\n",
        "# ReLU activation\n",
        "class Activation_ReLU :\n",
        "  # Forward pass\n",
        "  def forward( self , inputs ):\n",
        "    # Remember input values\n",
        "    self.inputs = inputs\n",
        "    # Calculate output values from inputs\n",
        "    self.output = np.maximum( 0 , inputs)\n",
        "\n",
        "  # Backward pass\n",
        "  def backward( self , dvalues ):\n",
        "    # Since we need to modify the original variable,\n",
        "    # let's make a copy of the values first\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Zero gradient where input values were negative\n",
        "    self.dinputs[self.inputs <= 0 ] = 0\n",
        "\n",
        "# Softmax activation\n",
        "class Activation_Softmax :\n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Get unnormalized probabilities\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis = 1 , keepdims = True ))\n",
        "\n",
        "    # Normalize them for each sample\n",
        "    probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True )\n",
        "    \n",
        "    self.output = probabilities\n",
        "\n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues ):\n",
        "    # Create uninitialized array\n",
        "    self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "    # Enumerate outputs and gradients\n",
        "    for index, (single_output, single_dvalues) in enumerate ( zip (self.output, dvalues)):\n",
        "      # Flatten output array\n",
        "      single_output = single_output.reshape( - 1 , 1 )\n",
        "\n",
        "      # Calculate Jacobian matrix of the output and\n",
        "      jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "\n",
        "      # Calculate sample-wise gradient\n",
        "      # and add it to the array of sample gradients\n",
        "      self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
        "\n",
        "# SGD optimizer\n",
        "class Optimizer_SGD :\n",
        "  # Initialize optimizer - set settings,\n",
        "  # learning rate of 1. is default for this optimizer\n",
        "  def __init__ ( self , learning_rate = 1. , decay = 0. , momentum = 0. ):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.momentum = momentum\n",
        "\n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params ( self ):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * ( 1. / ( 1. + self.decay * self.iterations))\n",
        "\n",
        "  # Update parameters\n",
        "  def update_params ( self , layer ):\n",
        "    # If we use momentum\n",
        "    if self.momentum:\n",
        "      # If layer does not contain momentum arrays, create them\n",
        "      # filled with zeros\n",
        "      if not hasattr (layer, 'weight_momentums'):\n",
        "        layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "        # If there is no momentum array for weights\n",
        "        # The array doesn't exist for biases yet either.\n",
        "        layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "      # Build weight updates with momentum - take previous\n",
        "      # updates multiplied by retain factor and update with\n",
        "      # current gradients\n",
        "      weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
        "      layer.weight_momentums = weight_updates\n",
        "\n",
        "      # Build bias updates\n",
        "      bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
        "      layer.bias_momentums = bias_updates\n",
        "    # Vanilla SGD updates (as before momentum update)\n",
        "    else :\n",
        "      weight_updates = - self.current_learning_rate * layer.dweights\n",
        "      bias_updates = - self.current_learning_rate * layer.dbiases\n",
        "    \n",
        "    # Update weights and biases using either\n",
        "    # vanilla or momentum updates\n",
        "    layer.weights += weight_updates\n",
        "    layer.biases += bias_updates\n",
        "      \n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params ( self ):\n",
        "    self.iterations += 1\n",
        "\n",
        "# Adagrad optimizer\n",
        "class Optimizer_Adagrad :\n",
        "  # Initialize optimizer - set settings\n",
        "  def __init__ ( self , learning_rate = 1. , decay = 0. , epsilon = 1e-7 ):\n",
        "      self.learning_rate = learning_rate\n",
        "      self.current_learning_rate = learning_rate\n",
        "      self.decay = decay\n",
        "      self.iterations = 0\n",
        "      self.epsilon = epsilon\n",
        "\n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params ( self ):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * ( 1. / ( 1. + self.decay * self.iterations))\n",
        "  \n",
        "  # Update parameters\n",
        "  def update_params ( self , layer ):\n",
        "    # If layer does not contain cache arrays,\n",
        "    # create them filled with zeros\n",
        "    if not hasattr (layer, 'weight_cache' ):\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "    # Update cache with squared current gradients\n",
        "    layer.weight_cache += layer.dweights ** 2\n",
        "    layer.bias_cache += layer.dbiases ** 2\n",
        "\n",
        "    # Vanilla SGD parameter update + normalization\n",
        "    # with square rooted cache\n",
        "    layer.weights += - self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "    layer.biases += - self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params ( self ):\n",
        "    self.iterations += 1\n",
        "\n",
        "# RMSprop optimizer\n",
        "class Optimizer_RMSprop :\n",
        "  # Initialize optimizer - set settings\n",
        "  def __init__ ( self , learning_rate = 0.001 , decay = 0. , epsilon = 1e-7 , rho = 0.9 ):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "    self.rho = rho\n",
        "\n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params ( self ):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * ( 1. / ( 1. + self.decay * self.iterations))\n",
        "\n",
        "  # Update parameters\n",
        "  def update_params ( self , layer ):\n",
        "    # If layer does not contain cache arrays,\n",
        "    # create them filled with zeros\n",
        "    if not hasattr (layer, 'weight_cache' ):\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "    # Update cache with squared current gradients\n",
        "    layer.weight_cache = self.rho * layer.weight_cache + ( 1 - self.rho) * layer.dweights ** 2\n",
        "    layer.bias_cache = self.rho * layer.bias_cache + ( 1 - self.rho) * layer.dbiases ** 2\n",
        "\n",
        "    # Vanilla SGD parameter update + normalization\n",
        "    # with square rooted cache\n",
        "    layer.weights += - self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "    layer.biases += - self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params ( self ):\n",
        "    self.iterations += 1\n",
        "\n",
        "\n",
        "# Adam optimizer\n",
        "class Optimizer_Adam :\n",
        "  # Initialize optimizer - set settings\n",
        "  def __init__ ( self , learning_rate = 0.001 , decay = 0. , epsilon = 1e-7 ,\n",
        "    beta_1 = 0.9 , beta_2 = 0.999 ):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "\n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params ( self ):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * ( 1. / ( 1. + self.decay * self.iterations))\n",
        "\n",
        "  # Update parameters\n",
        "  def update_params ( self , layer ):\n",
        "    # If layer does not contain cache arrays,\n",
        "    # create them filled with zeros\n",
        "    if not hasattr (layer, 'weight_cache' ):\n",
        "      layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "    # Update momentum with current gradients\n",
        "    layer.weight_momentums = self.beta_1 * layer.weight_momentums + ( 1 - self.beta_1) * layer.dweights\n",
        "    layer.bias_momentums = self.beta_1 * layer.bias_momentums + ( 1 - self.beta_1) * layer.dbiases\n",
        "\n",
        "    # Get corrected momentum\n",
        "    # self.iteration is 0 at first pass\n",
        "    # and we need to start with 1 here\n",
        "    weight_momentums_corrected = layer.weight_momentums / ( 1 - self.beta_1 ** (self.iterations + 1 ))\n",
        "    bias_momentums_corrected = layer.bias_momentums / ( 1 - self.beta_1 ** (self.iterations + 1 ))\n",
        "\n",
        "    # Update cache with squared current gradients\n",
        "    layer.weight_cache = self.beta_2 * layer.weight_cache + ( 1 - self.beta_2) * layer.dweights ** 2\n",
        "    layer.bias_cache = self.beta_2 * layer.bias_cache + ( 1 - self.beta_2) * layer.dbiases ** 2\n",
        "\n",
        "    # Get corrected cache\n",
        "    weight_cache_corrected = layer.weight_cache / ( 1 - self.beta_2 ** (self.iterations + 1 ))\n",
        "    bias_cache_corrected = layer.bias_cache / ( 1 - self.beta_2 ** (self.iterations + 1 ))\n",
        "\n",
        "    # Vanilla SGD parameter update + normalization\n",
        "    # with square rooted cache\n",
        "    layer.weights += - self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
        "    layer.biases += - self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
        "\n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params ( self ):\n",
        "    self.iterations += 1\n",
        "\n",
        "# Common loss class\n",
        "class Loss :\n",
        "  # Regularization loss calculation\n",
        "  def regularization_loss ( self , layer ):\n",
        "    # 0 by default\n",
        "    regularization_loss = 0\n",
        "\n",
        "    # L1 regularization - weights\n",
        "    # calculate only when factor greater than 0\n",
        "    if layer.weight_regularizer_l1 > 0 :\n",
        "      regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
        "\n",
        "    # L2 regularization - weights\n",
        "    if layer.weight_regularizer_l2 > 0 :\n",
        "      regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
        "\n",
        "    # L1 regularization - biases\n",
        "    # calculate only when factor greater than 0\n",
        "    if layer.bias_regularizer_l1 > 0 :\n",
        "      regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
        "\n",
        "    # L2 regularization - biases\n",
        "    if layer.bias_regularizer_l2 > 0 :\n",
        "      regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
        "\n",
        "    return regularization_loss\n",
        "\n",
        "  # Calculates the data and regularization losses\n",
        "  # given model output and ground truth values\n",
        "  def calculate ( self , output , y ):\n",
        "    # Calculate sample losses\n",
        "    sample_losses = self.forward(output, y)\n",
        "\n",
        "    # Calculate mean loss\n",
        "    data_loss = np.mean(sample_losses)\n",
        "\n",
        "    # Return loss\n",
        "    return data_loss\n",
        "\n",
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy ( Loss ):\n",
        "  # Forward pass\n",
        "  def forward( self , y_pred , y_true ):\n",
        "    # Number of samples in a batch\n",
        "    samples = len(y_pred)\n",
        "\n",
        "    # Clip data to prevent division by 0\n",
        "    # Clip both sides to not drag mean towards any value\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7 , 1 - 1e-7 )\n",
        "\n",
        "    # Probabilities for target values -\n",
        "    # only if categorical labels\n",
        "    if len(y_true.shape) == 1 :\n",
        "      correct_confidences = y_pred_clipped[range(samples),y_true]\n",
        "    elif len(y_true.shape) == 2 : # Mask values - only for one-hot encoded labels\n",
        "      correct_confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
        "\n",
        "    # Losses\n",
        "    negative_log_likelihoods = - np.log(correct_confidences)\n",
        "\n",
        "    return negative_log_likelihoods\n",
        "\n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues , y_true ):\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "\n",
        "    # Number of labels in every sample\n",
        "    # We'll use the first sample to count them\n",
        "    labels = len(dvalues[ 0 ])\n",
        "\n",
        "    # If labels are sparse, turn them into one-hot vector\n",
        "    if len(y_true.shape) == 1 :\n",
        "      y_true = np.eye(labels)[y_true]\n",
        "\n",
        "    # Calculate gradient\n",
        "    self.dinputs = - y_true / dvalues\n",
        "\n",
        "    # Normalize gradient\n",
        "    self.dinputs = self.dinputs / samples\n",
        "\n",
        "# Softmax classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy ():\n",
        "  # Creates activation and loss function objects\n",
        "  def __init__( self ):\n",
        "    self.activation = Activation_Softmax()\n",
        "    self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "  # Forward pass\n",
        "  def forward( self , inputs , y_true ):\n",
        "    # Output layer's activation function\n",
        "    self.activation.forward(inputs)\n",
        "    # Set the output\n",
        "    self.output = self.activation.output\n",
        "    # Calculate and return loss value\n",
        "    return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "  # Backward pass\n",
        "  def backward( self , dvalues , y_true ):\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "\n",
        "    # If labels are one-hot encoded,\n",
        "    # turn them into discrete values\n",
        "    if len(y_true.shape) == 2 :\n",
        "      y_true = np.argmax(y_true, axis = 1)\n",
        "\n",
        "    # Copy so we can safely modify\n",
        "    self.dinputs = dvalues.copy()\n",
        "\n",
        "    # Calculate gradient\n",
        "    self.dinputs[ range (samples), y_true] -= 1\n",
        "\n",
        "    # Normalize gradient\n",
        "    self.dinputs = self.dinputs / samples"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Zk6pHznyZvf",
        "outputId": "aa8d8310-3687-4963-c855-2a18935255af"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data( samples = 1000 , classes = 3 )\n",
        "\n",
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense( 2 , 512 , weight_regularizer_l2 = 5e-4 , bias_regularizer_l2 = 5e-4 )\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create dropout layer\n",
        "dropout1 = Layer_Dropout( 0.1 )\n",
        "\n",
        "# Create second Dense layer with 64 input features (as we take output\n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense( 512 , 3 )\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_Adam( learning_rate = 0.05 , decay = 5e-5 )\n",
        "\n",
        "# Train in loop\n",
        "for epoch in range ( 10001 ):\n",
        "  # Perform a forward pass of our training data through this layer\n",
        "  dense1.forward(X)\n",
        "\n",
        "  # Perform a forward pass through activation function\n",
        "  # takes the output of first dense layer here\n",
        "  activation1.forward(dense1.output)\n",
        "\n",
        "  # Perform a forward pass through Dropout layer\n",
        "  dropout1.forward(activation1.output)\n",
        "\n",
        "  # Perform a forward pass through second Dense layer\n",
        "  # takes outputs of activation function of first layer as inputs\n",
        "  dense2.forward(dropout1.output)\n",
        "\n",
        "  # Perform a forward pass through the activation/loss function\n",
        "  # takes the output of second dense layer here and returns loss\n",
        "  data_loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "  # Calculate regularization penalty\n",
        "  regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2)\n",
        "\n",
        "  # Calculate overall loss\n",
        "  loss = data_loss + regularization_loss\n",
        "\n",
        "  # Calculate accuracy from output of activation2 and targets\n",
        "  # calculate values along first axis\n",
        "  predictions = np.argmax(loss_activation.output, axis = 1 )\n",
        "\n",
        "  if len (y.shape) == 2 :\n",
        "    y = np.argmax(y, axis = 1 )\n",
        "\n",
        "  accuracy = np.mean(predictions == y)\n",
        "\n",
        "  if not epoch % 100 :\n",
        "    print ( 'epoch: '+ str(epoch) + ' , ' +\n",
        "           'acc: ' + str(accuracy) + ' , ' +\n",
        "           'loss: ' + str(loss)  + ' , ' +\n",
        "           'data_loss: ' + str(data_loss)  + ' , ' +\n",
        "           'reg_loss: ' + str(regularization_loss)  + ' , ' +\n",
        "           'lr: ' + str(optimizer.current_learning_rate) )\n",
        "\n",
        "  # Backward pass\n",
        "  loss_activation.backward(loss_activation.output, y)\n",
        "  dense2.backward(loss_activation.dinputs)\n",
        "  dropout1.backward(dense2.dinputs)\n",
        "  activation1.backward(dropout1.dinputs)\n",
        "  dense1.backward(activation1.dinputs)\n",
        "  # Update weights and biases\n",
        "\n",
        "  optimizer.pre_update_params()\n",
        "  optimizer.update_params(dense1)\n",
        "  optimizer.update_params(dense2)\n",
        "  optimizer.post_update_params()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 , acc: 0.2986666666666667 , loss: 1.098708974994719 , data_loss: 1.0986584 , reg_loss: 5.0532497465610504e-05 , lr: 0.05\n",
            "epoch: 100 , acc: 0.7263333333333334 , loss: 0.7150272212028503 , data_loss: 0.65628594 , reg_loss: 0.058741279602050785 , lr: 0.04975371909050202\n",
            "epoch: 200 , acc: 0.789 , loss: 0.6422347893714905 , data_loss: 0.5660904 , reg_loss: 0.07614438438415527 , lr: 0.049507401356502806\n",
            "epoch: 300 , acc: 0.806 , loss: 0.582886121749878 , data_loss: 0.50211763 , reg_loss: 0.08076848793029785 , lr: 0.0492635105177595\n",
            "epoch: 400 , acc: 0.8183333333333334 , loss: 0.5554092724323273 , data_loss: 0.47467306 , reg_loss: 0.08073620986938476 , lr: 0.04902201088288642\n",
            "epoch: 500 , acc: 0.8376666666666667 , loss: 0.5178370723724366 , data_loss: 0.4386289 , reg_loss: 0.07920816040039064 , lr: 0.048782867456949125\n",
            "epoch: 600 , acc: 0.8433333333333334 , loss: 0.4933849337100983 , data_loss: 0.4162064 , reg_loss: 0.07717854404449463 , lr: 0.04854604592455945\n",
            "epoch: 700 , acc: 0.84 , loss: 0.5025210845470428 , data_loss: 0.42414543 , reg_loss: 0.07837565422058106 , lr: 0.048311512633460556\n",
            "epoch: 800 , acc: 0.847 , loss: 0.49883904933929446 , data_loss: 0.42480624 , reg_loss: 0.07403281211853027 , lr: 0.04807923457858551\n",
            "epoch: 900 , acc: 0.85 , loss: 0.48454825735092166 , data_loss: 0.41188437 , reg_loss: 0.07266388988494873 , lr: 0.04784917938657352\n",
            "epoch: 1000 , acc: 0.8406666666666667 , loss: 0.477479455947876 , data_loss: 0.4077474 , reg_loss: 0.06973206806182862 , lr: 0.04762131530072861\n",
            "epoch: 1100 , acc: 0.8486666666666667 , loss: 0.489291517496109 , data_loss: 0.41886392 , reg_loss: 0.0704275951385498 , lr: 0.04739561116640599\n",
            "epoch: 1200 , acc: 0.8376666666666667 , loss: 0.4857159833908081 , data_loss: 0.41781652 , reg_loss: 0.06789946365356445 , lr: 0.04717203641681212\n",
            "epoch: 1300 , acc: 0.8446666666666667 , loss: 0.4949256870746612 , data_loss: 0.42667988 , reg_loss: 0.06824580764770508 , lr: 0.04695056105920466\n",
            "epoch: 1400 , acc: 0.8503333333333334 , loss: 0.47813075709342956 , data_loss: 0.41099402 , reg_loss: 0.06713673400878907 , lr: 0.04673115566147951\n",
            "epoch: 1500 , acc: 0.833 , loss: 0.5098083264827729 , data_loss: 0.4444562 , reg_loss: 0.06535213661193848 , lr: 0.046513791339132055\n",
            "epoch: 1600 , acc: 0.837 , loss: 0.4844174385070801 , data_loss: 0.42064404 , reg_loss: 0.06377339363098145 , lr: 0.04629843974258068\n",
            "epoch: 1700 , acc: 0.8553333333333333 , loss: 0.4645580270290375 , data_loss: 0.4004365 , reg_loss: 0.06412153625488282 , lr: 0.046085073044840774\n",
            "epoch: 1800 , acc: 0.8583333333333333 , loss: 0.46709677219390866 , data_loss: 0.40262973 , reg_loss: 0.06446703910827636 , lr: 0.04587366392953806\n",
            "epoch: 1900 , acc: 0.8436666666666667 , loss: 0.47091380548477174 , data_loss: 0.4085768 , reg_loss: 0.062337018966674804 , lr: 0.04566418557925019\n",
            "epoch: 2000 , acc: 0.8373333333333334 , loss: 0.4798859043121338 , data_loss: 0.4177692 , reg_loss: 0.062116710662841794 , lr: 0.045456611664166556\n",
            "epoch: 2100 , acc: 0.8396666666666667 , loss: 0.5068989732265472 , data_loss: 0.44528386 , reg_loss: 0.06161511325836182 , lr: 0.045250916331055706\n",
            "epoch: 2200 , acc: 0.8636666666666667 , loss: 0.45900350308418275 , data_loss: 0.39657584 , reg_loss: 0.06242766475677491 , lr: 0.0450470741925312\n",
            "epoch: 2300 , acc: 0.8636666666666667 , loss: 0.4469137785434723 , data_loss: 0.38649598 , reg_loss: 0.060417800903320315 , lr: 0.04484506031660612\n",
            "epoch: 2400 , acc: 0.8676666666666667 , loss: 0.44249372577667234 , data_loss: 0.38366687 , reg_loss: 0.05882685279846192 , lr: 0.04464485021652753\n",
            "epoch: 2500 , acc: 0.8613333333333333 , loss: 0.4449452323913574 , data_loss: 0.38634348 , reg_loss: 0.05860175323486328 , lr: 0.044446419840881816\n",
            "epoch: 2600 , acc: 0.8523333333333334 , loss: 0.45846092224121093 , data_loss: 0.3986863 , reg_loss: 0.05977463245391846 , lr: 0.04424974556396301\n",
            "epoch: 2700 , acc: 0.8553333333333333 , loss: 0.45897578620910645 , data_loss: 0.3959074 , reg_loss: 0.06306838417053223 , lr: 0.04405480417639544\n",
            "epoch: 2800 , acc: 0.8623333333333333 , loss: 0.44308047580718996 , data_loss: 0.38270628 , reg_loss: 0.06037419128417969 , lr: 0.04386157287600334\n",
            "epoch: 2900 , acc: 0.87 , loss: 0.4203444836139679 , data_loss: 0.35963705 , reg_loss: 0.06070743179321289 , lr: 0.04367002925891961\n",
            "epoch: 3000 , acc: 0.8726666666666667 , loss: 0.4414391963481903 , data_loss: 0.3832719 , reg_loss: 0.05816729354858398 , lr: 0.043480151310926564\n",
            "epoch: 3100 , acc: 0.853 , loss: 0.4574299507141113 , data_loss: 0.39895344 , reg_loss: 0.058476512908935546 , lr: 0.04329191739902161\n",
            "epoch: 3200 , acc: 0.8483333333333334 , loss: 0.4454608306884766 , data_loss: 0.38854778 , reg_loss: 0.05691305255889893 , lr: 0.043105306263201\n",
            "epoch: 3300 , acc: 0.8656666666666667 , loss: 0.44623773074150086 , data_loss: 0.38862845 , reg_loss: 0.057609277725219725 , lr: 0.0429202970084553\n",
            "epoch: 3400 , acc: 0.8616666666666667 , loss: 0.4337886345386505 , data_loss: 0.37750247 , reg_loss: 0.056286163330078125 , lr: 0.04273686909696996\n",
            "epoch: 3500 , acc: 0.833 , loss: 0.46687125945091246 , data_loss: 0.41077408 , reg_loss: 0.056097177505493166 , lr: 0.04255500234052514\n",
            "epoch: 3600 , acc: 0.8656666666666667 , loss: 0.451722038269043 , data_loss: 0.3956995 , reg_loss: 0.05602253723144532 , lr: 0.042374676893088686\n",
            "epoch: 3700 , acc: 0.8546666666666667 , loss: 0.4821521279811859 , data_loss: 0.42477295 , reg_loss: 0.057379179954528806 , lr: 0.042195873243596776\n",
            "epoch: 3800 , acc: 0.8796666666666667 , loss: 0.4247666029930115 , data_loss: 0.36752385 , reg_loss: 0.05724275398254394 , lr: 0.04201857220891634\n",
            "epoch: 3900 , acc: 0.8626666666666667 , loss: 0.41865706062316893 , data_loss: 0.36391747 , reg_loss: 0.054739590644836425 , lr: 0.041842754926984395\n",
            "epoch: 4000 , acc: 0.865 , loss: 0.44033393478393557 , data_loss: 0.38591146 , reg_loss: 0.05442247009277344 , lr: 0.04166840285011875\n",
            "epoch: 4100 , acc: 0.868 , loss: 0.42443215918540955 , data_loss: 0.3686814 , reg_loss: 0.05575075817108155 , lr: 0.041495497738495375\n",
            "epoch: 4200 , acc: 0.8636666666666667 , loss: 0.44779077339172363 , data_loss: 0.39307535 , reg_loss: 0.05471542644500733 , lr: 0.041324021653787346\n",
            "epoch: 4300 , acc: 0.8593333333333333 , loss: 0.451344274520874 , data_loss: 0.39743662 , reg_loss: 0.05390765571594238 , lr: 0.041153956952961035\n",
            "epoch: 4400 , acc: 0.868 , loss: 0.4268691334724426 , data_loss: 0.3705265 , reg_loss: 0.05634264087677002 , lr: 0.040985286282224684\n",
            "epoch: 4500 , acc: 0.872 , loss: 0.414605073928833 , data_loss: 0.35805726 , reg_loss: 0.05654781341552735 , lr: 0.04081799257112535\n",
            "epoch: 4600 , acc: 0.8723333333333333 , loss: 0.40602095699310303 , data_loss: 0.35291016 , reg_loss: 0.053110795974731444 , lr: 0.04065205902678971\n",
            "epoch: 4700 , acc: 0.8686666666666667 , loss: 0.43878009390830996 , data_loss: 0.38540992 , reg_loss: 0.05337017250061035 , lr: 0.04048746912830479\n",
            "epoch: 4800 , acc: 0.8616666666666667 , loss: 0.4247808096408844 , data_loss: 0.37204304 , reg_loss: 0.05273776626586914 , lr: 0.04032420662123473\n",
            "epoch: 4900 , acc: 0.8663333333333333 , loss: 0.4133035480976105 , data_loss: 0.36161605 , reg_loss: 0.05168750286102295 , lr: 0.04016225551226957\n",
            "epoch: 5000 , acc: 0.864 , loss: 0.4199795122146607 , data_loss: 0.36542547 , reg_loss: 0.05455404472351075 , lr: 0.04000160006400256\n",
            "epoch: 5100 , acc: 0.8623333333333333 , loss: 0.45268944931030275 , data_loss: 0.39827263 , reg_loss: 0.054416815757751466 , lr: 0.039842224789832265\n",
            "epoch: 5200 , acc: 0.8693333333333333 , loss: 0.42550645780563356 , data_loss: 0.37144274 , reg_loss: 0.05406372261047363 , lr: 0.03968411444898608\n",
            "epoch: 5300 , acc: 0.8703333333333333 , loss: 0.40810341453552246 , data_loss: 0.35623848 , reg_loss: 0.05186493015289307 , lr: 0.03952725404166173\n",
            "epoch: 5400 , acc: 0.874 , loss: 0.39718092894554136 , data_loss: 0.34517846 , reg_loss: 0.05200247383117676 , lr: 0.03937162880428363\n",
            "epoch: 5500 , acc: 0.8666666666666667 , loss: 0.4127813503742218 , data_loss: 0.36024204 , reg_loss: 0.05253931140899658 , lr: 0.03921722420487078\n",
            "epoch: 5600 , acc: 0.8573333333333333 , loss: 0.43054248952865604 , data_loss: 0.37766773 , reg_loss: 0.05287476444244385 , lr: 0.03906402593851323\n",
            "epoch: 5700 , acc: 0.8683333333333333 , loss: 0.4250163154602051 , data_loss: 0.37299323 , reg_loss: 0.052023084640502926 , lr: 0.038912019922954205\n",
            "epoch: 5800 , acc: 0.868 , loss: 0.41472454833984373 , data_loss: 0.36312056 , reg_loss: 0.0516039924621582 , lr: 0.038761192294274965\n",
            "epoch: 5900 , acc: 0.863 , loss: 0.4202600829601288 , data_loss: 0.36859724 , reg_loss: 0.05166284370422363 , lr: 0.038611529402679645\n",
            "epoch: 6000 , acc: 0.8666666666666667 , loss: 0.4336824779510498 , data_loss: 0.3825605 , reg_loss: 0.05112198638916016 , lr: 0.03846301780837725\n",
            "epoch: 6100 , acc: 0.877 , loss: 0.3966850712299347 , data_loss: 0.344353 , reg_loss: 0.05233208084106445 , lr: 0.03831564427755853\n",
            "epoch: 6200 , acc: 0.8666666666666667 , loss: 0.40585721683502196 , data_loss: 0.35454786 , reg_loss: 0.05130935859680176 , lr: 0.03816939577846483\n",
            "epoch: 6300 , acc: 0.8673333333333333 , loss: 0.4193989861011505 , data_loss: 0.3681698 , reg_loss: 0.05122917175292969 , lr: 0.038024259477546674\n",
            "epoch: 6400 , acc: 0.8646666666666667 , loss: 0.40619928812980655 , data_loss: 0.3555288 , reg_loss: 0.05067048645019531 , lr: 0.03788022273570969\n",
            "epoch: 6500 , acc: 0.8756666666666667 , loss: 0.403401437997818 , data_loss: 0.3536897 , reg_loss: 0.04971173763275147 , lr: 0.03773727310464546\n",
            "epoch: 6600 , acc: 0.8676666666666667 , loss: 0.4391376588344574 , data_loss: 0.37677893 , reg_loss: 0.06235872840881348 , lr: 0.03759539832324524\n",
            "epoch: 6700 , acc: 0.8683333333333333 , loss: 0.43005720019340515 , data_loss: 0.37037572 , reg_loss: 0.0596814775466919 , lr: 0.03745458631409416\n",
            "epoch: 6800 , acc: 0.8766666666666667 , loss: 0.40366033697128295 , data_loss: 0.34675878 , reg_loss: 0.056901554107666015 , lr: 0.03731482518004403\n",
            "epoch: 6900 , acc: 0.877 , loss: 0.4107616832256317 , data_loss: 0.35615382 , reg_loss: 0.054607867240905765 , lr: 0.03717610320086248\n",
            "epoch: 7000 , acc: 0.8723333333333333 , loss: 0.40733424162864684 , data_loss: 0.3547772 , reg_loss: 0.05255705451965332 , lr: 0.03703840882995667\n",
            "epoch: 7100 , acc: 0.845 , loss: 0.4783767099380493 , data_loss: 0.42610323 , reg_loss: 0.05227347564697266 , lr: 0.036901730691169414\n",
            "epoch: 7200 , acc: 0.8693333333333333 , loss: 0.4139236662387848 , data_loss: 0.36247602 , reg_loss: 0.05144764518737793 , lr: 0.03676605757564617\n",
            "epoch: 7300 , acc: 0.8566666666666667 , loss: 0.42768645572662356 , data_loss: 0.3774799 , reg_loss: 0.05020654487609864 , lr: 0.03663137843877066\n",
            "epoch: 7400 , acc: 0.8703333333333333 , loss: 0.4035111105442047 , data_loss: 0.35400143 , reg_loss: 0.04950967788696289 , lr: 0.03649768239716778\n",
            "epoch: 7500 , acc: 0.8606666666666667 , loss: 0.43275712871551514 , data_loss: 0.38412845 , reg_loss: 0.048628677368164064 , lr: 0.03636495872577185\n",
            "epoch: 7600 , acc: 0.8803333333333333 , loss: 0.3943531279563904 , data_loss: 0.34545273 , reg_loss: 0.0489004020690918 , lr: 0.03623319685495851\n",
            "epoch: 7700 , acc: 0.8756666666666667 , loss: 0.4221660511493683 , data_loss: 0.3727974 , reg_loss: 0.04936865139007569 , lr: 0.03610238636773891\n",
            "epoch: 7800 , acc: 0.8703333333333333 , loss: 0.4280288825035095 , data_loss: 0.3791831 , reg_loss: 0.048845768928527836 , lr: 0.03597251699701428\n",
            "epoch: 7900 , acc: 0.8586666666666667 , loss: 0.42148810386657715 , data_loss: 0.37212396 , reg_loss: 0.0493641471862793 , lr: 0.035843578622889706\n",
            "epoch: 8000 , acc: 0.8803333333333333 , loss: 0.38978719305992127 , data_loss: 0.34056005 , reg_loss: 0.04922714424133301 , lr: 0.03571556127004536\n",
            "epoch: 8100 , acc: 0.8753333333333333 , loss: 0.40397749495506285 , data_loss: 0.35591277 , reg_loss: 0.048064720153808596 , lr: 0.03558845510516389\n",
            "epoch: 8200 , acc: 0.863 , loss: 0.4090780487060547 , data_loss: 0.36093986 , reg_loss: 0.04813818836212159 , lr: 0.03546225043441257\n",
            "epoch: 8300 , acc: 0.8706666666666667 , loss: 0.41550623631477357 , data_loss: 0.36661223 , reg_loss: 0.048894010543823245 , lr: 0.035336937700978836\n",
            "epoch: 8400 , acc: 0.8743333333333333 , loss: 0.4137180414199829 , data_loss: 0.3654598 , reg_loss: 0.048258241653442384 , lr: 0.03521250748265784\n",
            "epoch: 8500 , acc: 0.8686666666666667 , loss: 0.40071113967895505 , data_loss: 0.35245752 , reg_loss: 0.04825361633300781 , lr: 0.035088950489490865\n",
            "epoch: 8600 , acc: 0.8666666666666667 , loss: 0.4206826949119568 , data_loss: 0.37152237 , reg_loss: 0.04916032791137696 , lr: 0.0349662575614532\n",
            "epoch: 8700 , acc: 0.8653333333333333 , loss: 0.43180431246757506 , data_loss: 0.38360068 , reg_loss: 0.048203630447387694 , lr: 0.034844419666190465\n",
            "epoch: 8800 , acc: 0.8496666666666667 , loss: 0.42517424607276916 , data_loss: 0.37746647 , reg_loss: 0.04770777606964111 , lr: 0.034723427896801974\n",
            "epoch: 8900 , acc: 0.8766666666666667 , loss: 0.3913014314174652 , data_loss: 0.34301147 , reg_loss: 0.048289962768554684 , lr: 0.03460327346967023\n",
            "epoch: 9000 , acc: 0.87 , loss: 0.4030043003559113 , data_loss: 0.35513547 , reg_loss: 0.04786882972717286 , lr: 0.034483947722335255\n",
            "epoch: 9100 , acc: 0.877 , loss: 0.3962859995365143 , data_loss: 0.34958044 , reg_loss: 0.046705562591552736 , lr: 0.034365442111412764\n",
            "epoch: 9200 , acc: 0.862 , loss: 0.41845772171020507 , data_loss: 0.37061524 , reg_loss: 0.047842477798461916 , lr: 0.03424774821055516\n",
            "epoch: 9300 , acc: 0.8413333333333334 , loss: 0.4734694774150848 , data_loss: 0.42582974 , reg_loss: 0.04763973903656006 , lr: 0.03413085770845422\n",
            "epoch: 9400 , acc: 0.8696666666666667 , loss: 0.42779204869270326 , data_loss: 0.37941065 , reg_loss: 0.04838139438629151 , lr: 0.034014762406884586\n",
            "epoch: 9500 , acc: 0.8703333333333333 , loss: 0.40285760259628295 , data_loss: 0.3549022 , reg_loss: 0.04795539474487305 , lr: 0.03389945421878708\n",
            "epoch: 9600 , acc: 0.8666666666666667 , loss: 0.4143246874809265 , data_loss: 0.36686414 , reg_loss: 0.047460542678833005 , lr: 0.033784925166390756\n",
            "epoch: 9700 , acc: 0.864 , loss: 0.4339383680820465 , data_loss: 0.38665345 , reg_loss: 0.047284914970397954 , lr: 0.03367116737937304\n",
            "epoch: 9800 , acc: 0.872 , loss: 0.4004651234149933 , data_loss: 0.352505 , reg_loss: 0.04796012496948242 , lr: 0.033558173093056816\n",
            "epoch: 9900 , acc: 0.8683333333333333 , loss: 0.43114011526107787 , data_loss: 0.38463706 , reg_loss: 0.046503057479858396 , lr: 0.0334459346466437\n",
            "epoch: 10000 , acc: 0.8726666666666667 , loss: 0.44069694685935973 , data_loss: 0.3929225 , reg_loss: 0.04777445602416992 , lr: 0.03333444448148271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xO0xDhqy2T-",
        "outputId": "b61e420f-e015-4d60-9d16-a2cb5a69f4f5"
      },
      "source": [
        "# Validate the model\n",
        "\n",
        "# Create test dataset\n",
        "X_test, y_test = spiral_data( samples = 100 , classes = 3 )\n",
        "\n",
        "# Perform a forward pass of our testing data through this layer\n",
        "dense1.forward(X_test)\n",
        "\n",
        "# Perform a forward pass through activation function\n",
        "# takes the output of first dense layer here\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "# Perform a forward pass through second Dense layer\n",
        "# takes outputs of activation function of first layer as inputs\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "# Perform a forward pass through the activation/loss function\n",
        "# takes the output of second dense layer here and returns loss\n",
        "loss = loss_activation.forward(dense2.output, y_test)\n",
        "\n",
        "# Calculate accuracy from output of activation2 and targets\n",
        "# calculate values along first axis\n",
        "predictions = np.argmax(loss_activation.output, axis = 1 )\n",
        "\n",
        "if len (y_test.shape) == 2 :\n",
        "  y_test = np.argmax(y_test, axis = 1 )\n",
        "\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "\n",
        "print('validation, acc: ' + str(accuracy) + ', loss: ' + str(loss))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation, acc: 0.8633333333333333, loss: 0.5140036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ASS5l5nzqPn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}