{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Networks_from_Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbZy48Ocdxot",
        "outputId": "b85b2b2b-cddf-4426-9bdf-92ba2f63ecc5"
      },
      "source": [
        "inputs = [ 1 , 2 , 3 , 2.5 ]\n",
        "\n",
        "weights = [[ 0.2 , 0.8 , - 0.5 , 1 ],\n",
        "           [ 0.5 , - 0.91 , 0.26 , - 0.5 ],\n",
        "           [ - 0.26 , - 0.27 , 0.17 , 0.87 ]]\n",
        "\n",
        "biases = [ 2 , 3 , 0.5 ]\n",
        "\n",
        "# Output of current layer\n",
        "layer_outputs = []\n",
        "\n",
        "# For each neuron\n",
        "for neuron_weights, neuron_bias in zip (weights, biases):\n",
        "  # Zeroed output of given neuron\n",
        "  neuron_output = 0\n",
        "  # For each input and weight to the neuron\n",
        "  for n_input, weight in zip (inputs, neuron_weights):\n",
        "    # Multiply this input by associated weight\n",
        "    # and add to the neuron’s output variable\n",
        "    neuron_output += n_input * weight\n",
        "  # Add bias\n",
        "  neuron_output += neuron_bias\n",
        "  # Put neuron’s result to the layer’s output list\n",
        "  layer_outputs.append(neuron_output)\n",
        "print (layer_outputs)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.8, 1.21, 2.385]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pseRamnVyepV"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a5yNphYpVL3"
      },
      "source": [
        "A Single Neuron with NumPy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "bxSF2SeYePM4",
        "outputId": "318a754d-b141-4352-9f30-c516854dcc88"
      },
      "source": [
        "import numpy as np\n",
        "outputs = np.dot(weights, inputs) + bias\n",
        "print (outputs)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-5a6a58e3e123>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'bias' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS9Gaygpyf5p"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbv8QCOixm3Y"
      },
      "source": [
        "Multiple layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF3mfE9GpWwg"
      },
      "source": [
        "inputs = [[ 1 , 2 , 3 , 2.5 ],\n",
        "[ 2. , 5. , - 1. , 2 ],\n",
        "[ - 1.5 , 2.7 , 3.3 , - 0.8 ]]\n",
        "weights = [[ 0.2 , 0.8 , - 0.5 , 1 ],\n",
        "[ 0.5 , - 0.91 , 0.26 , - 0.5 ],\n",
        "[ - 0.26 , - 0.27 , 0.17 , 0.87 ]]\n",
        "biases = [ 2 , 3 , 0.5 ]\n",
        "weights2 = [[ 0.1 , - 0.14 , 0.5 ],\n",
        "[ - 0.5 , 0.12 , - 0.33 ],\n",
        "[ - 0.44 , 0.73 , - 0.13 ]]\n",
        "biases2 = [ - 1 , 2 , - 0.5 ]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntNfB8Pjx4z7",
        "outputId": "40afcb9d-5495-466c-dd70-f1eb8a7fef63"
      },
      "source": [
        "layer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
        "layer1_outputs.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A4GUYIxx6es",
        "outputId": "abbb68a5-0150-4ac6-b41f-91a197acf096"
      },
      "source": [
        "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n",
        "layer2_outputs.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok9HlVLVyc_c"
      },
      "source": [
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co3WxRLI05Rt"
      },
      "source": [
        "Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HiLYL0GyAxK",
        "outputId": "459fe4a9-ff12-436c-a65f-69a1fb8ab03f"
      },
      "source": [
        "!pip install nnfs"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nnfs\n",
            "  Downloading https://files.pythonhosted.org/packages/06/8c/3003a41d5229e65da792331b060dcad8100a0a5b9760f8c2074cde864148/nnfs-0.5.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nnfs) (1.19.5)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O4RmX-E0-DV"
      },
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "\n",
        "from nnfs.datasets import spiral_data\n",
        "nnfs.init()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb-fAk6f1P0X"
      },
      "source": [
        "The nnfs.init() does three things: it sets the random seed to 0 (by the default), creates a\n",
        "float32 dtype default, and overrides the original dot product from NumPy. All of these are meant\n",
        "to ensure repeatable results for following along."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "wMXNYlUS0_49",
        "outputId": "bf869e6f-288b-4a71-e66d-758b3e7e1a18"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "X, y = spiral_data( samples = 100 , classes = 3 )\n",
        "\n",
        "X.shape, y.shape\n",
        "\n",
        "plt.scatter(X[:, 0 ], X[:, 1 ], c = y, cmap = 'brg' )\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blgxY6zw16fW"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z71u9Ws1-bR"
      },
      "source": [
        "**Dense Layer Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nmdgqo-E1W9d"
      },
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "nnfs.init()\n",
        "# Dense layer\n",
        "class Layer_Dense :\n",
        "\n",
        "  # Layer initialization\n",
        "  def __init__ ( self , n_inputs , n_neurons ):\n",
        "    # Initialize weights and biases\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros(( 1 , n_neurons))\n",
        "\n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "  "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcssT3XP41kG",
        "outputId": "a36c8962-c63d-4aa5-abdd-c1b9ed9fb0f2"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data( samples = 100 , classes = 3 )\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Layer_Dense( 2 , 3 )\n",
        "\n",
        "# Perform a forward pass of our training data through this layer\n",
        "dense1.forward(X)\n",
        "\n",
        "# Let's see output of the first few samples:\n",
        "print (dense1.output[: 5 ])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(300, 2) (300,)\n",
            "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
            " [-1.0475188e-04  1.1395361e-04 -4.7983500e-05]\n",
            " [-2.7414842e-04  3.1729150e-04 -8.6921798e-05]\n",
            " [-4.2188365e-04  5.2666257e-04 -5.5912682e-05]\n",
            " [-5.7707680e-04  7.1401405e-04 -8.9430439e-05]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UmUkPzoL5e3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZQHcYujL7-f"
      },
      "source": [
        "**ReLU Activation Function Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCdR9ape49wt",
        "outputId": "e9d3675d-251b-4b4f-e8ce-8ade0d9fd07f"
      },
      "source": [
        "inputs = [ 0 , 2 , - 1 , 3.3 , - 2.7 , 1.1 , 2.2 , - 100 ]\n",
        "output = []\n",
        "for i in inputs:\n",
        "  output.append( max ( 0 , i))\n",
        "print (output)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pPdCfzHMCxM",
        "outputId": "d028ac86-830f-459b-b953-8bde101b12ef"
      },
      "source": [
        "import numpy as np\n",
        "inputs = [ 0 , 2 , - 1 , 3.3 , - 2.7 , 1.1 , 2.2 , - 100 ]\n",
        "output = np.maximum( 0 , inputs)\n",
        "print (output)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV-Zf7WKMTWV"
      },
      "source": [
        "# ReLU activation\n",
        "class Activation_ReLU :\n",
        "  \n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Calculate output values from input\n",
        "    self.output = np.maximum( 0 , inputs)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wlp-RuhMgEx",
        "outputId": "ff4b8590-0a66-4a53-f0ee-3bfdc4c3d86d"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data( samples = 100 , classes = 3 )\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Layer_Dense( 2 , 3 )\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Make a forward pass of our training data through this layer\n",
        "dense1.forward(X)\n",
        "\n",
        "# Forward pass through activation func.\n",
        "# Takes in output from previous layer\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "# Let's see output of the first few samples:\n",
        "print (activation1.output[: 5 ])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dktoRuWOEcpM"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHdaM_ryB5kM"
      },
      "source": [
        "The Softmax Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uljfUEnYMm6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48d8567f-65f5-4098-d98f-0671b4ada097"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Values from the earlier previous when we described\n",
        "# what a neural network is\n",
        "layer_outputs = [ 4.8 , 1.21 , 2.385 ]\n",
        "\n",
        "# For each value in a vector, calculate the exponential value\n",
        "exp_values = np.exp(layer_outputs)\n",
        "print ( 'exponentiated values:' )\n",
        "print (exp_values)\n",
        "\n",
        "# Now normalize values\n",
        "norm_values = exp_values / np.sum(exp_values)\n",
        "print ( 'normalized exponentiated values:' )\n",
        "\n",
        "print (norm_values)\n",
        "print ( 'sum of normalized values:' , np.sum(norm_values))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "exponentiated values:\n",
            "[121.51041752   3.35348465  10.85906266]\n",
            "normalized exponentiated values:\n",
            "[0.89528266 0.02470831 0.08000903]\n",
            "sum of normalized values: 0.9999999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FscckQWVCEfE"
      },
      "source": [
        "# Softmax activation\n",
        "class Activation_Softmax :\n",
        "\n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "\n",
        "    # Get unnormalized probabilities\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis = 1 , keepdims = True ))\n",
        "\n",
        "    # Normalize them for each sample\n",
        "    probabilities = exp_values / np.sum(exp_values, axis = 1 , keepdims = True )\n",
        "    \n",
        "    self.output = probabilities"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItYFuJcwEHKf",
        "outputId": "33152b86-718f-42e9-ecf4-ab48de435baf"
      },
      "source": [
        "softmax = Activation_Softmax()\n",
        "softmax.forward([[ 1 , 2 , 3 ]])\n",
        "print (softmax.output)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.09003057 0.24472847 0.66524096]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RR_tv5DEN_X",
        "outputId": "00771444-23b1-4bd1-bf3e-db2c44dd072e"
      },
      "source": [
        "softmax.forward([[ - 2 , - 1 , 0 ]]) # subtracted 3 - max from the list\n",
        "print (softmax.output)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.09003057 0.24472847 0.66524096]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx21Jto0Ef6U"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb04iDBuEV8W",
        "outputId": "3481b920-1618-41e3-a2a8-b7bd95fa00ed"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data( samples = 100 , classes = 3 )\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Layer_Dense( 2 , 3 )\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 3 input features (as we take output\n",
        "# of previous layer here) and 3 output values\n",
        "dense2 = Layer_Dense( 3 , 3 )\n",
        "\n",
        "# Create Softmax activation (to be used with Dense layer):\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "# Make a forward pass of our training data through this layer\n",
        "dense1.forward(X)\n",
        "print(dense1.output.shape)\n",
        "\n",
        "# Make a forward pass through activation function\n",
        "# it takes the output of first dense layer here\n",
        "activation1.forward(dense1.output)\n",
        "print(activation1.output.shape)\n",
        "\n",
        "# Make a forward pass through second Dense layer\n",
        "# it takes outputs of activation function of first layer as inputs\n",
        "dense2.forward(activation1.output)\n",
        "print(dense2.output.shape)\n",
        "\n",
        "# Make a forward pass through activation function\n",
        "# it takes the output of second dense layer here\n",
        "activation2.forward(dense2.output)\n",
        "print(activation2.output.shape)\n",
        "\n",
        "# Let's see output of the first few samples:\n",
        "print (activation2.output[: 5 ])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(300, 3)\n",
            "(300, 3)\n",
            "(300, 3)\n",
            "(300, 3)\n",
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.33333364 0.3333334  0.3333329 ]\n",
            " [0.33333385 0.33333346 0.33333266]\n",
            " [0.33333433 0.3333336  0.3333321 ]\n",
            " [0.33333465 0.33333373 0.33333164]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSjutWNMJLYE"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ-GlFiaJOWi"
      },
      "source": [
        "Full Code for Dense Layer and Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfNtfllzHlnm"
      },
      "source": [
        "import numpy as np\n",
        "import nnfs \n",
        "\n",
        "from nnfs.datasets import spiral_data\n",
        "nnfs.init()\n",
        "\n",
        "# Dense layer\n",
        "class Layer_Dense :\n",
        "  # Layer initialization\n",
        "  def __init__ ( self , n_inputs , n_neurons ):\n",
        "    # Initialize weights and biases\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros(( 1 , n_neurons))\n",
        "    # Forward pass\n",
        "\n",
        "  def forward ( self , inputs ):\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "# ReLU activation\n",
        "class Activation_ReLU :\n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Calculate output values from inputs\n",
        "    self.output = np.maximum( 0 , inputs)\n",
        "\n",
        "# Softmax activation\n",
        "class Activation_Softmax :\n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Get unnormalized probabilities\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis = 1 ,\n",
        "    keepdims = True ))\n",
        "    # Normalize them for each sample\n",
        "    probabilities = exp_values / np.sum(exp_values, axis = 1 ,\n",
        "    keepdims = True )\n",
        "    self.output = probabilities\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u5LVyr-JYLn",
        "outputId": "1d7d123e-b04d-47bc-d49d-6796bff99f31"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data( samples = 100 , classes = 3 )\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Layer_Dense( 2 , 3 )\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 3 input features (as we take output\n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense( 3 , 3 )\n",
        "\n",
        "# Create Softmax activation (to be used with Dense layer):\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "# Make a forward pass of our training data through this layer\n",
        "dense1.forward(X)\n",
        "\n",
        "# Make a forward pass through activation function\n",
        "# it takes the output of first dense layer here\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "# Make a forward pass through second Dense layer\n",
        "# it takes outputs of activation function of first layer as inputs\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "# Make a forward pass through activation function\n",
        "# it takes the output of second dense layer here\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "# Let's see output of the first few samples:\n",
        "print (activation2.output[: 5 ])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.3333332  0.3333332  0.33333364]\n",
            " [0.3333329  0.33333293 0.3333342 ]\n",
            " [0.3333326  0.33333263 0.33333477]\n",
            " [0.33333233 0.3333324  0.33333528]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkXJJ-zxME0Z"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F86HGLG2MHme"
      },
      "source": [
        "np.sum along different axis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql6V1DBKJ9QT",
        "outputId": "7f51562b-8041-4678-f7c2-9d2a17078c05"
      },
      "source": [
        "layer_outputs = np.array([[ 4.8 , 1.21 , 2.385 ],\n",
        "[ 8.9 , - 1.81 , 0.2 ],\n",
        "[ 1.41 , 1.051 , 0.026 ]])\n",
        "print ( 'Another way to think of it w/ a matrix == axis 0: columns:' )\n",
        "print (np.sum(layer_outputs, axis = 0, keepdims=True ))\n",
        "print ( 'So we can sum axis 1, but note the current shape:' )\n",
        "print (np.sum(layer_outputs, axis = 1, keepdims=True ))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Another way to think of it w/ a matrix == axis 0: columns:\n",
            "[[15.11   0.451  2.611]]\n",
            "So we can sum axis 1, but note the current shape:\n",
            "[[8.395]\n",
            " [7.29 ]\n",
            " [2.487]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FDhLBuXMNc8"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBy920b6yi3U"
      },
      "source": [
        "Categorical Cross-Entropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBGUnYOxLsZ2",
        "outputId": "9d4f7d7f-2ff8-4732-abaa-ab58b8090a25"
      },
      "source": [
        "softmax_outputs =  np.array([[ 0.7 , 0.1 , 0.2 ],\n",
        "                   [ 0.1 , 0.5 , 0.4 ],\n",
        "                   [ 0.02 , 0.9 , 0.08 ]])\n",
        "class_targets = [ 0 , 1 , 1 ]\n",
        "neg_log = -1 * np.log( softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
        "average_loss = np.mean(neg_log)\n",
        "print (average_loss)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.38506088005216804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX_uK5nEyyln"
      },
      "source": [
        "import numpy as np\n",
        "softmax_outputs = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
        "                            [ 0.1 , 0.5 , 0.4 ],\n",
        "                            [ 0.02 , 0.9 , 0.08 ]])\n",
        "class_targets = np.array([[ 1 , 0 , 0 ],\n",
        "                          [ 0 , 1 , 0 ],\n",
        "                          [ 0 , 1 , 0 ]])\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1Giv2w0zDZV",
        "outputId": "1f8ae72b-96c5-4a15-cb59-b9c5589c7431"
      },
      "source": [
        "# Probabilities for target values -\n",
        "# only if categorical labels\n",
        "if len(class_targets.shape) == 1 :\n",
        "  correct_confidences = softmax_outputs[range(len(softmax_outputs)),class_targets]\n",
        "elif len(class_targets.shape) == 2 :\n",
        "  correct_confidences = np.sum(softmax_outputs * class_targets,axis = 1)\n",
        "\n",
        "print(correct_confidences)\n",
        "# Losses\n",
        "neg_log = - np.log(correct_confidences)\n",
        "\n",
        "average_loss = np.mean(neg_log)\n",
        "\n",
        "print (average_loss)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.7 0.5 0.9]\n",
            "0.38506088005216804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBZuFZ4o3D2s"
      },
      "source": [
        "**Categorical Cross-Entropy Loss Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNrSFm_d0Unu"
      },
      "source": [
        "# Common loss class\n",
        "class Loss :\n",
        "  \n",
        "  # Calculates the data and regularization losses\n",
        "  # given model output and ground truth values\n",
        "  def calculate ( self , output , y ):\n",
        "    # Calculate sample losses\n",
        "    sample_losses = self.forward(output, y)\n",
        "\n",
        "    # Calculate mean loss\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    \n",
        "    # Return loss\n",
        "    return data_loss"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNQGIKoi31tM"
      },
      "source": [
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy ( Loss ):\n",
        "  \n",
        "  # Forward pass\n",
        "  def forward( self , y_pred , y_true ):\n",
        "    # Number of samples in a batch\n",
        "    samples = len(y_pred)\n",
        "\n",
        "    # Clip data to prevent division by 0\n",
        "    # Clip both sides to not drag mean towards any value\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7 , 1 - 1e-7 )\n",
        "\n",
        "    # Probabilities for target values -\n",
        "    # only if categorical labels\n",
        "    if len(y_true.shape) == 1 :\n",
        "      correct_confidences = y_pred_clipped[range(samples),y_true]\n",
        "    elif len(y_true.shape) == 2 : # Mask values - only for one-hot encoded labels\n",
        "      correct_confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
        "\n",
        "    # Losses\n",
        "    negative_log_likelihoods = - np.log(correct_confidences)\n",
        "\n",
        "    return negative_log_likelihoods"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC8-UPsl6IoF",
        "outputId": "1ae8aafc-3746-4df0-dc7e-ead19c333eb7"
      },
      "source": [
        "softmax_outputs = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
        "                            [ 0.1 , 0.5 , 0.4 ],\n",
        "                            [ 0.02 , 0.9 , 0.08 ]])\n",
        "class_targets = np.array([[ 1 , 0 , 0 ],\n",
        "                          [ 0 , 1 , 0 ],\n",
        "                          [ 0 , 1 , 0 ]])\n",
        "\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "\n",
        "loss = loss_function.calculate(softmax_outputs, class_targets)\n",
        "\n",
        "print (loss)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.38506088005216804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA1n7IeN8C2R"
      },
      "source": [
        "Summarize:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-sPgvn171F-"
      },
      "source": [
        "import numpy as np\n",
        "import nnfs \n",
        "\n",
        "from nnfs.datasets import spiral_data\n",
        "nnfs.init()\n",
        "\n",
        "# Dense layer\n",
        "class Layer_Dense :\n",
        "  # Layer initialization\n",
        "  def __init__ ( self , n_inputs , n_neurons ):\n",
        "    # Initialize weights and biases\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros(( 1 , n_neurons))\n",
        "    # Forward pass\n",
        "\n",
        "  def forward ( self , inputs ):\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "# ReLU activation\n",
        "class Activation_ReLU :\n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Calculate output values from inputs\n",
        "    self.output = np.maximum( 0 , inputs)\n",
        "\n",
        "# Softmax activation\n",
        "class Activation_Softmax :\n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Get unnormalized probabilities\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis = 1 , keepdims = True ))\n",
        "\n",
        "    # Normalize them for each sample\n",
        "    probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True )\n",
        "    \n",
        "    self.output = probabilities\n",
        "\n",
        "# Common loss class\n",
        "class Loss :  \n",
        "  # Calculates the data and regularization losses\n",
        "  # given model output and ground truth values\n",
        "  def calculate ( self , output , y ):\n",
        "    # Calculate sample losses\n",
        "    sample_losses = self.forward(output, y)\n",
        "\n",
        "    # Calculate mean loss\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    \n",
        "    # Return loss\n",
        "    return data_loss\n",
        "\n",
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy ( Loss ):\n",
        "  # Forward pass\n",
        "  def forward( self , y_pred , y_true ):\n",
        "    # Number of samples in a batch\n",
        "    samples = len(y_pred)\n",
        "\n",
        "    # Clip data to prevent division by 0\n",
        "    # Clip both sides to not drag mean towards any value\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7 , 1 - 1e-7 )\n",
        "\n",
        "    # Probabilities for target values -\n",
        "    # only if categorical labels\n",
        "    if len(y_true.shape) == 1 :\n",
        "      correct_confidences = y_pred_clipped[range(samples),y_true]\n",
        "    elif len(y_true.shape) == 2 : # Mask values - only for one-hot encoded labels\n",
        "      correct_confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
        "\n",
        "    # Losses\n",
        "    negative_log_likelihoods = - np.log(correct_confidences)\n",
        "\n",
        "    return negative_log_likelihoods"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ4TiHGQ8SBA",
        "outputId": "19755068-68ce-4c86-f407-ccd4d4a2e868"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data( samples = 100 , classes = 3 )\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Layer_Dense( 2 , 3 )\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 3 input features (as we take output\n",
        "# of previous layer here) and 3 output values\n",
        "dense2 = Layer_Dense( 3 , 3 )\n",
        "\n",
        "# Create Softmax activation (to be used with Dense layer):\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "# Create loss function\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Perform a forward pass of our training data through this layer\n",
        "dense1.forward(X)\n",
        "\n",
        "# Perform a forward pass through activation function\n",
        "# it takes the output of first dense layer here\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "# Perform a forward pass through second Dense layer\n",
        "# it takes outputs of activation function of first layer as inputs\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "# Perform a forward pass through activation function\n",
        "# it takes the output of second dense layer here\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "# Let's see output of the first few samples:\n",
        "print (activation2.output[: 5 ])\n",
        "\n",
        "# Perform a forward pass through loss function\n",
        "# it takes the output of second dense layer here and returns loss\n",
        "loss = loss_function.calculate(activation2.output, y)\n",
        "\n",
        "# Calculate accuracy from output of activation2 and targets\n",
        "# calculate values along first axis\n",
        "predictions = np.argmax(activation2.output, axis = 1 )\n",
        "\n",
        "if len (y.shape) == 2 :\n",
        "  y = np.argmax(y, axis=1)\n",
        "\n",
        "accuracy = np.mean(predictions == y)\n",
        "\n",
        "# Print accuracy\n",
        "print ( 'acc:' , accuracy)\n",
        "\n",
        "# Print loss value\n",
        "print ( 'loss:' , loss)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.3333332  0.3333332  0.33333364]\n",
            " [0.3333329  0.33333293 0.3333342 ]\n",
            " [0.3333326  0.33333263 0.33333477]\n",
            " [0.33333233 0.3333324  0.33333528]]\n",
            "acc: 0.34\n",
            "loss: 1.0986104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plON56MuBHX6"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxxNQSCEBJUL"
      },
      "source": [
        "**Accuracy Calculation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t01WPKE5822X",
        "outputId": "f15295d5-a7ab-4d5e-c812-4f17853bd00e"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Probabilities of 3 samples\n",
        "softmax_outputs = np.array([[ 0.7 , 0.2 , 0.1 ], \n",
        "                            [ 0.5 , 0.1 , 0.4 ], \n",
        "                            [ 0.02 , 0.9 , 0.08 ]])\n",
        "\n",
        "# Target (ground-truth) labels for 3 samples\n",
        "class_targets = np.array([ 0 , 1 , 1 ])\n",
        "\n",
        "# Calculate values along second axis (axis of index 1)\n",
        "predictions = np.argmax(softmax_outputs, axis = 1)\n",
        "\n",
        "# If targets are one-hot encoded - convert them\n",
        "if len(class_targets.shape) == 2 :\n",
        "  class_targets = np.argmax(class_targets, axis = 1 )\n",
        "\n",
        "# True evaluates to 1; False to 0\n",
        "accuracy = np.mean(predictions == class_targets)\n",
        "\n",
        "print ( 'acc:' , accuracy)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc: 0.6666666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vv3mTIUEeXW"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XofQDVfEhjj"
      },
      "source": [
        "**Introducing Optimization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT3d2HDxBXOF"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import nnfs\n",
        "from nnfs.datasets import vertical_data\n",
        "nnfs.init()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "tSa12jliEpbT",
        "outputId": "154b3c43-0a58-44c7-fc30-823c61257f19"
      },
      "source": [
        "X, y = vertical_data( samples = 100 , classes = 3 )\n",
        "plt.scatter(X[:, 0 ], X[:, 1 ], c = y, s = 40 , cmap = 'brg' )\n",
        "plt.show()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSukeBJvEq_o"
      },
      "source": [
        "# Create dataset\n",
        "X, y = vertical_data( samples = 100 , classes = 3 )\n",
        "\n",
        "# Create model\n",
        "dense1 = Layer_Dense( 2 , 3 ) # first dense layer, 2 inputs\n",
        "\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dense2 = Layer_Dense( 3 , 3 ) # second dense layer, 3 inputs, 3 outputs\n",
        "\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "# Create loss function\n",
        "loss_function = Loss_CategoricalCrossentropy()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqqRzCdFE4_k"
      },
      "source": [
        "# Helper variables\n",
        "lowest_loss = 9999999 # some initial value\n",
        "best_dense1_weights = dense1.weights.copy()\n",
        "best_dense1_biases = dense1.biases.copy()\n",
        "best_dense2_weights = dense2.weights.copy()\n",
        "best_dense2_biases = dense2.biases.copy()"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJzNifTYGWT7"
      },
      "source": [
        "Updating weights and biases randomly and checking the loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDQVqxcNFbTi",
        "outputId": "9a5292a3-e724-46fc-be8b-83c5c06c20a6"
      },
      "source": [
        "for iteration in range ( 10000 ):\n",
        "  # Generate a new set of weights for iteration\n",
        "  dense1.weights = 0.05 * np.random.randn( 2 , 3 )\n",
        "  dense1.biases = 0.05 * np.random.randn( 1 , 3 )\n",
        "  dense2.weights = 0.05 * np.random.randn( 3 , 3 )\n",
        "  dense2.biases = 0.05 * np.random.randn( 1 , 3 )\n",
        "\n",
        "  # Perform a forward pass of the training data through this layer\n",
        "  dense1.forward(X)\n",
        "  activation1.forward(dense1.output)\n",
        "  dense2.forward(activation1.output)\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  # Perform a forward pass through activation function\n",
        "  # it takes the output of second dense layer here and returns loss\n",
        "  loss = loss_function.calculate(activation2.output, y)\n",
        "\n",
        "  # Calculate accuracy from output of activation2 and targets\n",
        "  # calculate values along first axis\n",
        "  predictions = np.argmax(activation2.output, axis = 1 )\n",
        "  accuracy = np.mean(predictions == y)\n",
        "\n",
        "  # If loss is smaller - print and save weights and biases aside\n",
        "  if loss < lowest_loss:\n",
        "    print ( 'New set of weights found, iteration:' , iteration, 'loss:' , loss, 'acc:' , accuracy)\n",
        "    best_dense1_weights = dense1.weights.copy()\n",
        "    best_dense1_biases = dense1.biases.copy()\n",
        "    best_dense2_weights = dense2.weights.copy()\n",
        "    best_dense2_biases = dense2.biases.copy()\n",
        "    lowest_loss = loss"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New set of weights found, iteration: 0 loss: 1.1016203 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 1 loss: 1.1002508 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 2 loss: 1.0992025 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 3 loss: 1.0986239 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 10 loss: 1.0984299 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 22 loss: 1.0976521 acc: 0.36333333333333334\n",
            "New set of weights found, iteration: 150 loss: 1.0974255 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 874 loss: 1.0972673 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 894 loss: 1.096895 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 1036 loss: 1.0954281 acc: 0.3333333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2BYlWHXGcyj"
      },
      "source": [
        "Instead of setting parameters with randomly-chosen values each iteration, apply a fraction of these values to parameters. With this, weights will be updated from what currently yields us the lowest loss instead of aimlessly randomly. If the adjustment decreases loss, we will make it the new point to adjust from. If loss instead increases due to the adjustment, then we will revert to the previous point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN4PQVLhF1IS",
        "outputId": "480c5167-7c4f-410c-ff1e-1292643a0789"
      },
      "source": [
        "for iteration in range ( 10000 ):\n",
        "  # Update weights with some small random values\n",
        "  dense1.weights += 0.05 * np.random.randn( 2 , 3 )\n",
        "  dense1.biases += 0.05 * np.random.randn( 1 , 3 )\n",
        "  dense2.weights += 0.05 * np.random.randn( 3 , 3 )\n",
        "  dense2.biases += 0.05 * np.random.randn( 1 , 3 )\n",
        "\n",
        "  # Perform a forward pass of the training data through this layer\n",
        "  dense1.forward(X)\n",
        "  activation1.forward(dense1.output)\n",
        "  dense2.forward(activation1.output)\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  # Perform a forward pass through activation function\n",
        "  # it takes the output of second dense layer here and returns loss\n",
        "  loss = loss_function.calculate(activation2.output, y)\n",
        "\n",
        "  # Calculate accuracy from output of activation2 and targets\n",
        "  # calculate values along first axis\n",
        "  predictions = np.argmax(activation2.output, axis = 1 )\n",
        "  accuracy = np.mean(predictions == y)\n",
        "\n",
        "  # If loss is smaller - print and save weights and biases aside\n",
        "  if loss < lowest_loss:\n",
        "    print ( 'New set of weights found, iteration:' , iteration, 'loss:' , loss, 'acc:' , accuracy)\n",
        "    best_dense1_weights = dense1.weights.copy()\n",
        "    best_dense1_biases = dense1.biases.copy()\n",
        "    best_dense2_weights = dense2.weights.copy()\n",
        "    best_dense2_biases = dense2.biases.copy()\n",
        "    lowest_loss = loss\n",
        "    # Revert weights and biases\n",
        "  else :\n",
        "    dense1.weights = best_dense1_weights.copy()\n",
        "    dense1.biases = best_dense1_biases.copy()\n",
        "    dense2.weights = best_dense2_weights.copy()\n",
        "    dense2.biases = best_dense2_biases.copy()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New set of weights found, iteration: 5 loss: 1.0946134 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 8 loss: 1.0942385 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 9 loss: 1.0933579 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 10 loss: 1.0894163 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 18 loss: 1.0869116 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 19 loss: 1.08632 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 20 loss: 1.0809617 acc: 0.66\n",
            "New set of weights found, iteration: 21 loss: 1.076914 acc: 0.42\n",
            "New set of weights found, iteration: 22 loss: 1.0763092 acc: 0.4266666666666667\n",
            "New set of weights found, iteration: 24 loss: 1.073875 acc: 0.6533333333333333\n",
            "New set of weights found, iteration: 25 loss: 1.0705098 acc: 0.6666666666666666\n",
            "New set of weights found, iteration: 28 loss: 1.0684985 acc: 0.6533333333333333\n",
            "New set of weights found, iteration: 29 loss: 1.0676612 acc: 0.6666666666666666\n",
            "New set of weights found, iteration: 30 loss: 1.0675691 acc: 0.6533333333333333\n",
            "New set of weights found, iteration: 31 loss: 1.0573452 acc: 0.5533333333333333\n",
            "New set of weights found, iteration: 36 loss: 1.0561142 acc: 0.5366666666666666\n",
            "New set of weights found, iteration: 37 loss: 1.0543473 acc: 0.36\n",
            "New set of weights found, iteration: 42 loss: 1.0511491 acc: 0.33666666666666667\n",
            "New set of weights found, iteration: 43 loss: 1.0406609 acc: 0.5966666666666667\n",
            "New set of weights found, iteration: 45 loss: 1.0313528 acc: 0.6433333333333333\n",
            "New set of weights found, iteration: 46 loss: 1.016273 acc: 0.6666666666666666\n",
            "New set of weights found, iteration: 47 loss: 1.0162238 acc: 0.6666666666666666\n",
            "New set of weights found, iteration: 53 loss: 1.0123385 acc: 0.6666666666666666\n",
            "New set of weights found, iteration: 61 loss: 1.011563 acc: 0.6466666666666666\n",
            "New set of weights found, iteration: 62 loss: 1.0073348 acc: 0.45\n",
            "New set of weights found, iteration: 65 loss: 1.0018284 acc: 0.6\n",
            "New set of weights found, iteration: 67 loss: 1.0014983 acc: 0.7166666666666667\n",
            "New set of weights found, iteration: 70 loss: 0.9907717 acc: 0.84\n",
            "New set of weights found, iteration: 72 loss: 0.9793341 acc: 0.7233333333333334\n",
            "New set of weights found, iteration: 74 loss: 0.9762207 acc: 0.44666666666666666\n",
            "New set of weights found, iteration: 75 loss: 0.9743563 acc: 0.5566666666666666\n",
            "New set of weights found, iteration: 80 loss: 0.9665206 acc: 0.73\n",
            "New set of weights found, iteration: 81 loss: 0.9588155 acc: 0.6666666666666666\n",
            "New set of weights found, iteration: 85 loss: 0.9481254 acc: 0.6133333333333333\n",
            "New set of weights found, iteration: 86 loss: 0.944082 acc: 0.81\n",
            "New set of weights found, iteration: 87 loss: 0.9368296 acc: 0.6733333333333333\n",
            "New set of weights found, iteration: 88 loss: 0.9356524 acc: 0.8966666666666666\n",
            "New set of weights found, iteration: 91 loss: 0.9301902 acc: 0.8733333333333333\n",
            "New set of weights found, iteration: 97 loss: 0.924804 acc: 0.6733333333333333\n",
            "New set of weights found, iteration: 105 loss: 0.9141502 acc: 0.67\n",
            "New set of weights found, iteration: 109 loss: 0.9039451 acc: 0.7\n",
            "New set of weights found, iteration: 112 loss: 0.88885516 acc: 0.6666666666666666\n",
            "New set of weights found, iteration: 120 loss: 0.88756186 acc: 0.6666666666666666\n",
            "New set of weights found, iteration: 121 loss: 0.8856074 acc: 0.71\n",
            "New set of weights found, iteration: 124 loss: 0.88279694 acc: 0.8466666666666667\n",
            "New set of weights found, iteration: 125 loss: 0.87917787 acc: 0.8633333333333333\n",
            "New set of weights found, iteration: 127 loss: 0.8728354 acc: 0.8366666666666667\n",
            "New set of weights found, iteration: 132 loss: 0.86008507 acc: 0.8466666666666667\n",
            "New set of weights found, iteration: 134 loss: 0.85817873 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 135 loss: 0.84843147 acc: 0.8766666666666667\n",
            "New set of weights found, iteration: 139 loss: 0.843858 acc: 0.8366666666666667\n",
            "New set of weights found, iteration: 141 loss: 0.8343647 acc: 0.6666666666666666\n",
            "New set of weights found, iteration: 147 loss: 0.8323578 acc: 0.61\n",
            "New set of weights found, iteration: 149 loss: 0.8259927 acc: 0.7033333333333334\n",
            "New set of weights found, iteration: 151 loss: 0.8223896 acc: 0.73\n",
            "New set of weights found, iteration: 153 loss: 0.81917095 acc: 0.8033333333333333\n",
            "New set of weights found, iteration: 154 loss: 0.81122667 acc: 0.7933333333333333\n",
            "New set of weights found, iteration: 157 loss: 0.80483955 acc: 0.71\n",
            "New set of weights found, iteration: 164 loss: 0.80331093 acc: 0.8366666666666667\n",
            "New set of weights found, iteration: 165 loss: 0.7943688 acc: 0.9\n",
            "New set of weights found, iteration: 169 loss: 0.7856384 acc: 0.8233333333333334\n",
            "New set of weights found, iteration: 171 loss: 0.77829355 acc: 0.8266666666666667\n",
            "New set of weights found, iteration: 174 loss: 0.77764344 acc: 0.7233333333333334\n",
            "New set of weights found, iteration: 175 loss: 0.76536435 acc: 0.7033333333333334\n",
            "New set of weights found, iteration: 178 loss: 0.76509637 acc: 0.7\n",
            "New set of weights found, iteration: 180 loss: 0.75828725 acc: 0.66\n",
            "New set of weights found, iteration: 181 loss: 0.74695057 acc: 0.6966666666666667\n",
            "New set of weights found, iteration: 184 loss: 0.73200023 acc: 0.7433333333333333\n",
            "New set of weights found, iteration: 192 loss: 0.7267019 acc: 0.85\n",
            "New set of weights found, iteration: 193 loss: 0.7113463 acc: 0.8566666666666667\n",
            "New set of weights found, iteration: 194 loss: 0.6930832 acc: 0.8366666666666667\n",
            "New set of weights found, iteration: 198 loss: 0.6861262 acc: 0.9166666666666666\n",
            "New set of weights found, iteration: 200 loss: 0.67658323 acc: 0.8366666666666667\n",
            "New set of weights found, iteration: 202 loss: 0.67329437 acc: 0.7466666666666667\n",
            "New set of weights found, iteration: 204 loss: 0.6705808 acc: 0.77\n",
            "New set of weights found, iteration: 205 loss: 0.65369374 acc: 0.8366666666666667\n",
            "New set of weights found, iteration: 207 loss: 0.6462688 acc: 0.77\n",
            "New set of weights found, iteration: 213 loss: 0.628924 acc: 0.7866666666666666\n",
            "New set of weights found, iteration: 216 loss: 0.62723553 acc: 0.85\n",
            "New set of weights found, iteration: 217 loss: 0.6214861 acc: 0.8566666666666667\n",
            "New set of weights found, iteration: 218 loss: 0.6140687 acc: 0.85\n",
            "New set of weights found, iteration: 224 loss: 0.6112883 acc: 0.8866666666666667\n",
            "New set of weights found, iteration: 227 loss: 0.6086789 acc: 0.8866666666666667\n",
            "New set of weights found, iteration: 237 loss: 0.60569113 acc: 0.8766666666666667\n",
            "New set of weights found, iteration: 239 loss: 0.59760517 acc: 0.8766666666666667\n",
            "New set of weights found, iteration: 240 loss: 0.59433556 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 241 loss: 0.5867646 acc: 0.91\n",
            "New set of weights found, iteration: 243 loss: 0.58238 acc: 0.8633333333333333\n",
            "New set of weights found, iteration: 245 loss: 0.5780778 acc: 0.85\n",
            "New set of weights found, iteration: 246 loss: 0.56868804 acc: 0.86\n",
            "New set of weights found, iteration: 248 loss: 0.5645385 acc: 0.8766666666666667\n",
            "New set of weights found, iteration: 251 loss: 0.5597818 acc: 0.89\n",
            "New set of weights found, iteration: 258 loss: 0.55714935 acc: 0.88\n",
            "New set of weights found, iteration: 260 loss: 0.5548784 acc: 0.89\n",
            "New set of weights found, iteration: 263 loss: 0.5513882 acc: 0.92\n",
            "New set of weights found, iteration: 270 loss: 0.5503325 acc: 0.8666666666666667\n",
            "New set of weights found, iteration: 271 loss: 0.5482467 acc: 0.8566666666666667\n",
            "New set of weights found, iteration: 273 loss: 0.5468023 acc: 0.87\n",
            "New set of weights found, iteration: 274 loss: 0.5463857 acc: 0.9\n",
            "New set of weights found, iteration: 279 loss: 0.5423891 acc: 0.8933333333333333\n",
            "New set of weights found, iteration: 280 loss: 0.5398152 acc: 0.91\n",
            "New set of weights found, iteration: 283 loss: 0.53563845 acc: 0.86\n",
            "New set of weights found, iteration: 284 loss: 0.53490096 acc: 0.87\n",
            "New set of weights found, iteration: 285 loss: 0.5318954 acc: 0.89\n",
            "New set of weights found, iteration: 288 loss: 0.52936864 acc: 0.88\n",
            "New set of weights found, iteration: 290 loss: 0.5257483 acc: 0.8966666666666666\n",
            "New set of weights found, iteration: 291 loss: 0.5225309 acc: 0.88\n",
            "New set of weights found, iteration: 304 loss: 0.5194027 acc: 0.8833333333333333\n",
            "New set of weights found, iteration: 310 loss: 0.51850885 acc: 0.8833333333333333\n",
            "New set of weights found, iteration: 315 loss: 0.51302874 acc: 0.8633333333333333\n",
            "New set of weights found, iteration: 320 loss: 0.50580263 acc: 0.9066666666666666\n",
            "New set of weights found, iteration: 321 loss: 0.5009886 acc: 0.92\n",
            "New set of weights found, iteration: 326 loss: 0.49963802 acc: 0.88\n",
            "New set of weights found, iteration: 328 loss: 0.49753052 acc: 0.8866666666666667\n",
            "New set of weights found, iteration: 332 loss: 0.49172068 acc: 0.9\n",
            "New set of weights found, iteration: 333 loss: 0.48417938 acc: 0.9166666666666666\n",
            "New set of weights found, iteration: 341 loss: 0.48239157 acc: 0.9\n",
            "New set of weights found, iteration: 346 loss: 0.48007506 acc: 0.8866666666666667\n",
            "New set of weights found, iteration: 349 loss: 0.47219035 acc: 0.8766666666666667\n",
            "New set of weights found, iteration: 350 loss: 0.4692621 acc: 0.8833333333333333\n",
            "New set of weights found, iteration: 357 loss: 0.46734324 acc: 0.8833333333333333\n",
            "New set of weights found, iteration: 359 loss: 0.46307555 acc: 0.88\n",
            "New set of weights found, iteration: 362 loss: 0.45919886 acc: 0.8733333333333333\n",
            "New set of weights found, iteration: 365 loss: 0.455842 acc: 0.9166666666666666\n",
            "New set of weights found, iteration: 368 loss: 0.44885743 acc: 0.9166666666666666\n",
            "New set of weights found, iteration: 370 loss: 0.44420633 acc: 0.9166666666666666\n",
            "New set of weights found, iteration: 374 loss: 0.44399557 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 376 loss: 0.4424771 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 379 loss: 0.43829712 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 381 loss: 0.4278239 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 382 loss: 0.4253224 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 384 loss: 0.42414188 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 387 loss: 0.42251545 acc: 0.9633333333333334\n",
            "New set of weights found, iteration: 388 loss: 0.41927347 acc: 0.96\n",
            "New set of weights found, iteration: 389 loss: 0.41611314 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 390 loss: 0.4133088 acc: 0.96\n",
            "New set of weights found, iteration: 391 loss: 0.4106471 acc: 0.95\n",
            "New set of weights found, iteration: 392 loss: 0.4062831 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 395 loss: 0.4019169 acc: 0.94\n",
            "New set of weights found, iteration: 398 loss: 0.40071824 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 401 loss: 0.3993213 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 406 loss: 0.3953862 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 409 loss: 0.39385858 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 411 loss: 0.39312792 acc: 0.94\n",
            "New set of weights found, iteration: 413 loss: 0.39222068 acc: 0.92\n",
            "New set of weights found, iteration: 414 loss: 0.39053118 acc: 0.92\n",
            "New set of weights found, iteration: 415 loss: 0.3777455 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 417 loss: 0.37405005 acc: 0.94\n",
            "New set of weights found, iteration: 418 loss: 0.37257966 acc: 0.95\n",
            "New set of weights found, iteration: 419 loss: 0.36818516 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 421 loss: 0.3649284 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 422 loss: 0.36032602 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 425 loss: 0.35796502 acc: 0.95\n",
            "New set of weights found, iteration: 429 loss: 0.35739335 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 438 loss: 0.3521092 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 440 loss: 0.34854797 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 446 loss: 0.34227905 acc: 0.95\n",
            "New set of weights found, iteration: 451 loss: 0.3389237 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 463 loss: 0.33432898 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 464 loss: 0.33031315 acc: 0.96\n",
            "New set of weights found, iteration: 467 loss: 0.32995284 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 471 loss: 0.32913128 acc: 0.95\n",
            "New set of weights found, iteration: 475 loss: 0.31915763 acc: 0.9633333333333334\n",
            "New set of weights found, iteration: 477 loss: 0.31823227 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 478 loss: 0.31752834 acc: 0.95\n",
            "New set of weights found, iteration: 481 loss: 0.3072321 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 484 loss: 0.30291507 acc: 0.9633333333333334\n",
            "New set of weights found, iteration: 487 loss: 0.30269936 acc: 0.9633333333333334\n",
            "New set of weights found, iteration: 489 loss: 0.30045483 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 493 loss: 0.29755086 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 505 loss: 0.29523176 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 510 loss: 0.2928856 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 518 loss: 0.29198572 acc: 0.94\n",
            "New set of weights found, iteration: 519 loss: 0.2877223 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 520 loss: 0.28726786 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 521 loss: 0.27968222 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 527 loss: 0.27459133 acc: 0.95\n",
            "New set of weights found, iteration: 528 loss: 0.27451083 acc: 0.95\n",
            "New set of weights found, iteration: 536 loss: 0.26903224 acc: 0.96\n",
            "New set of weights found, iteration: 538 loss: 0.2656899 acc: 0.96\n",
            "New set of weights found, iteration: 540 loss: 0.2651617 acc: 0.95\n",
            "New set of weights found, iteration: 543 loss: 0.2632482 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 548 loss: 0.2628083 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 557 loss: 0.25931275 acc: 0.9633333333333334\n",
            "New set of weights found, iteration: 562 loss: 0.25615215 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 564 loss: 0.2535411 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 574 loss: 0.24998316 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 575 loss: 0.2431017 acc: 0.9633333333333334\n",
            "New set of weights found, iteration: 582 loss: 0.24042292 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 587 loss: 0.23934516 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 588 loss: 0.23849419 acc: 0.9633333333333334\n",
            "New set of weights found, iteration: 599 loss: 0.23720011 acc: 0.96\n",
            "New set of weights found, iteration: 610 loss: 0.2358406 acc: 0.9633333333333334\n",
            "New set of weights found, iteration: 612 loss: 0.23229909 acc: 0.95\n",
            "New set of weights found, iteration: 613 loss: 0.23044482 acc: 0.9633333333333334\n",
            "New set of weights found, iteration: 615 loss: 0.22488597 acc: 0.96\n",
            "New set of weights found, iteration: 628 loss: 0.22126569 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 630 loss: 0.21821903 acc: 0.9633333333333334\n",
            "New set of weights found, iteration: 635 loss: 0.2177285 acc: 0.95\n",
            "New set of weights found, iteration: 636 loss: 0.21647345 acc: 0.95\n",
            "New set of weights found, iteration: 638 loss: 0.2158234 acc: 0.9633333333333334\n",
            "New set of weights found, iteration: 639 loss: 0.21578766 acc: 0.96\n",
            "New set of weights found, iteration: 640 loss: 0.21191801 acc: 0.96\n",
            "New set of weights found, iteration: 644 loss: 0.20981646 acc: 0.9633333333333334\n",
            "New set of weights found, iteration: 645 loss: 0.20680578 acc: 0.9633333333333334\n",
            "New set of weights found, iteration: 649 loss: 0.2053648 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 651 loss: 0.20505805 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 656 loss: 0.20476326 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 657 loss: 0.20309097 acc: 0.95\n",
            "New set of weights found, iteration: 661 loss: 0.2012857 acc: 0.9633333333333334\n",
            "New set of weights found, iteration: 663 loss: 0.20115453 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 665 loss: 0.1992378 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 668 loss: 0.1982705 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 669 loss: 0.19327141 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 670 loss: 0.19234532 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 673 loss: 0.19040808 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 677 loss: 0.18814385 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 681 loss: 0.18715121 acc: 0.95\n",
            "New set of weights found, iteration: 683 loss: 0.18667725 acc: 0.9633333333333334\n",
            "New set of weights found, iteration: 685 loss: 0.1845321 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 691 loss: 0.18426114 acc: 0.95\n",
            "New set of weights found, iteration: 700 loss: 0.18268766 acc: 0.95\n",
            "New set of weights found, iteration: 701 loss: 0.17770763 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 716 loss: 0.17652863 acc: 0.9633333333333334\n",
            "New set of weights found, iteration: 717 loss: 0.17514962 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 722 loss: 0.17383417 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 725 loss: 0.17364793 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 727 loss: 0.1730582 acc: 0.96\n",
            "New set of weights found, iteration: 728 loss: 0.17211315 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 731 loss: 0.17116784 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 738 loss: 0.1710644 acc: 0.96\n",
            "New set of weights found, iteration: 747 loss: 0.17039804 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 748 loss: 0.16763942 acc: 0.9633333333333334\n",
            "New set of weights found, iteration: 752 loss: 0.16484174 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 766 loss: 0.16320767 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 768 loss: 0.16282675 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 771 loss: 0.16197549 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 775 loss: 0.1585353 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 783 loss: 0.15763351 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 785 loss: 0.15724704 acc: 0.9633333333333334\n",
            "New set of weights found, iteration: 788 loss: 0.15302402 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 789 loss: 0.1510435 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 800 loss: 0.15073809 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 807 loss: 0.1495439 acc: 0.96\n",
            "New set of weights found, iteration: 809 loss: 0.14771433 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 815 loss: 0.14668612 acc: 0.96\n",
            "New set of weights found, iteration: 824 loss: 0.1443763 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 829 loss: 0.1427285 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 834 loss: 0.1425845 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 842 loss: 0.1419948 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 844 loss: 0.14172903 acc: 0.96\n",
            "New set of weights found, iteration: 846 loss: 0.13887571 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 847 loss: 0.138732 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 852 loss: 0.137612 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 876 loss: 0.13730425 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 891 loss: 0.13698028 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 894 loss: 0.1364112 acc: 0.95\n",
            "New set of weights found, iteration: 903 loss: 0.13635674 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 927 loss: 0.13583048 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 934 loss: 0.13551655 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 939 loss: 0.13534538 acc: 0.95\n",
            "New set of weights found, iteration: 945 loss: 0.13488568 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 946 loss: 0.13425244 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 948 loss: 0.13316315 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 960 loss: 0.13249697 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 963 loss: 0.13245702 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 964 loss: 0.13177715 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 991 loss: 0.13166565 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1001 loss: 0.13132642 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 1005 loss: 0.13122414 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 1031 loss: 0.13020171 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 1032 loss: 0.1298156 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1037 loss: 0.12901267 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1038 loss: 0.12866028 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 1039 loss: 0.12796205 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1057 loss: 0.12765326 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1065 loss: 0.12720902 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1068 loss: 0.12685438 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1071 loss: 0.12630343 acc: 0.95\n",
            "New set of weights found, iteration: 1078 loss: 0.1251204 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1098 loss: 0.12510322 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1100 loss: 0.12452881 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1104 loss: 0.12418078 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 1112 loss: 0.12362457 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1113 loss: 0.12350471 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1122 loss: 0.12318901 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1128 loss: 0.12310295 acc: 0.95\n",
            "New set of weights found, iteration: 1133 loss: 0.1217305 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1165 loss: 0.12068993 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 1171 loss: 0.120348014 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1173 loss: 0.12009351 acc: 0.96\n",
            "New set of weights found, iteration: 1175 loss: 0.11946898 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1190 loss: 0.1194503 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1195 loss: 0.11930039 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1204 loss: 0.11921445 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1206 loss: 0.1190989 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1239 loss: 0.11900724 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1250 loss: 0.118847504 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1258 loss: 0.11867984 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1267 loss: 0.11854496 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1273 loss: 0.11846283 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1302 loss: 0.118066505 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1308 loss: 0.11775387 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1326 loss: 0.117663585 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1343 loss: 0.1176523 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 1388 loss: 0.11751311 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 1390 loss: 0.117273904 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1399 loss: 0.117133 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1411 loss: 0.11694186 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1500 loss: 0.11687138 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1555 loss: 0.11641548 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 1627 loss: 0.11638132 acc: 0.96\n",
            "New set of weights found, iteration: 1651 loss: 0.116303556 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1671 loss: 0.11618017 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1678 loss: 0.11604979 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1745 loss: 0.11600502 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1820 loss: 0.1159856 acc: 0.96\n",
            "New set of weights found, iteration: 1836 loss: 0.11576009 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1840 loss: 0.11575737 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1896 loss: 0.11572957 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1924 loss: 0.11566905 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1937 loss: 0.11555337 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1949 loss: 0.11545283 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 1955 loss: 0.115377694 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 2036 loss: 0.11525354 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 2362 loss: 0.115212984 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 3226 loss: 0.11517309 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 4044 loss: 0.115137026 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 4070 loss: 0.11512782 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 4231 loss: 0.115107015 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 4468 loss: 0.11507187 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 4736 loss: 0.115056455 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 6124 loss: 0.11503342 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 6273 loss: 0.1150239 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 6538 loss: 0.11502083 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 6541 loss: 0.115008496 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 6600 loss: 0.11484965 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 6939 loss: 0.114787966 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 6973 loss: 0.114784814 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 7031 loss: 0.11470741 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 7052 loss: 0.114697926 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 7222 loss: 0.11468651 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 7819 loss: 0.11466564 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 7897 loss: 0.11461661 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 8014 loss: 0.11454796 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 8044 loss: 0.114541754 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 8129 loss: 0.11454091 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 8141 loss: 0.11444425 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 8382 loss: 0.11437045 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 8656 loss: 0.11433159 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 8666 loss: 0.114325106 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 8772 loss: 0.114224754 acc: 0.9566666666666667\n",
            "New set of weights found, iteration: 9648 loss: 0.11415145 acc: 0.9566666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odThtPtBH8ed"
      },
      "source": [
        "Full Algorithm to search for best weights and biases by searching around the best weights found based on loss and accuracy rather that checking at random weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-jInY1dHE2-",
        "outputId": "1c9196b3-5032-4375-eff6-f80d4e1ce4d3"
      },
      "source": [
        "# Create dataset\n",
        "X, y = vertical_data( samples = 100 , classes = 3 )\n",
        "\n",
        "# Create model\n",
        "dense1 = Layer_Dense( 2 , 3 ) # first dense layer, 2 inputs\n",
        "\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dense2 = Layer_Dense( 3 , 3 ) # second dense layer, 3 inputs, 3 outputs\n",
        "\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "# Create loss function\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Helper variables\n",
        "lowest_loss = 9999999 # some initial value\n",
        "best_dense1_weights = dense1.weights.copy()\n",
        "best_dense1_biases = dense1.biases.copy()\n",
        "best_dense2_weights = dense2.weights.copy()\n",
        "best_dense2_biases = dense2.biases.copy()\n",
        "\n",
        "for iteration in range ( 10000 ):\n",
        "  # Update weights with some small random values\n",
        "  dense1.weights += 0.05 * np.random.randn( 2 , 3 )\n",
        "  dense1.biases += 0.05 * np.random.randn( 1 , 3 )\n",
        "  dense2.weights += 0.05 * np.random.randn( 3 , 3 )\n",
        "  dense2.biases += 0.05 * np.random.randn( 1 , 3 )\n",
        "\n",
        "  # Perform a forward pass of the training data through this layer\n",
        "  dense1.forward(X)\n",
        "  activation1.forward(dense1.output)\n",
        "  dense2.forward(activation1.output)\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  # Perform a forward pass through activation function\n",
        "  # it takes the output of second dense layer here and returns loss\n",
        "  loss = loss_function.calculate(activation2.output, y)\n",
        "\n",
        "  # Calculate accuracy from output of activation2 and targets\n",
        "  # calculate values along first axis\n",
        "  predictions = np.argmax(activation2.output, axis = 1 )\n",
        "  accuracy = np.mean(predictions == y)\n",
        "\n",
        "  # If loss is smaller - print and save weights and biases aside\n",
        "  if loss < lowest_loss:\n",
        "    print ( 'New set of weights found, iteration:' , iteration, 'loss:' , loss, 'acc:' , accuracy)\n",
        "    best_dense1_weights = dense1.weights.copy()\n",
        "    best_dense1_biases = dense1.biases.copy()\n",
        "    best_dense2_weights = dense2.weights.copy()\n",
        "    best_dense2_biases = dense2.biases.copy()\n",
        "    lowest_loss = loss\n",
        "    # Revert weights and biases\n",
        "  else :\n",
        "    dense1.weights = best_dense1_weights.copy()\n",
        "    dense1.biases = best_dense1_biases.copy()\n",
        "    dense2.weights = best_dense2_weights.copy()\n",
        "    dense2.biases = best_dense2_biases.copy()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New set of weights found, iteration: 0 loss: 1.0984097 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 4 loss: 1.0977073 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 6 loss: 1.097061 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 8 loss: 1.0951436 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 10 loss: 1.0932177 acc: 0.62\n",
            "New set of weights found, iteration: 12 loss: 1.0915836 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 13 loss: 1.0896236 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 14 loss: 1.0872676 acc: 0.6\n",
            "New set of weights found, iteration: 20 loss: 1.0855696 acc: 0.34\n",
            "New set of weights found, iteration: 21 loss: 1.0843276 acc: 0.5933333333333334\n",
            "New set of weights found, iteration: 24 loss: 1.0827562 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 28 loss: 1.0767611 acc: 0.5833333333333334\n",
            "New set of weights found, iteration: 30 loss: 1.0756652 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 31 loss: 1.0716627 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 33 loss: 1.0695561 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 34 loss: 1.0667007 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 35 loss: 1.0598183 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 36 loss: 1.0565299 acc: 0.3333333333333333\n",
            "New set of weights found, iteration: 37 loss: 1.0534073 acc: 0.38333333333333336\n",
            "New set of weights found, iteration: 40 loss: 1.0409307 acc: 0.5566666666666666\n",
            "New set of weights found, iteration: 41 loss: 1.0265149 acc: 0.34\n",
            "New set of weights found, iteration: 43 loss: 1.0234596 acc: 0.39666666666666667\n",
            "New set of weights found, iteration: 45 loss: 1.0233771 acc: 0.35\n",
            "New set of weights found, iteration: 47 loss: 1.022231 acc: 0.36666666666666664\n",
            "New set of weights found, iteration: 52 loss: 1.0093712 acc: 0.37\n",
            "New set of weights found, iteration: 54 loss: 1.0048933 acc: 0.4666666666666667\n",
            "New set of weights found, iteration: 55 loss: 1.0023605 acc: 0.34\n",
            "New set of weights found, iteration: 59 loss: 0.99279666 acc: 0.4033333333333333\n",
            "New set of weights found, iteration: 60 loss: 0.9887915 acc: 0.5333333333333333\n",
            "New set of weights found, iteration: 62 loss: 0.9840864 acc: 0.5866666666666667\n",
            "New set of weights found, iteration: 63 loss: 0.9696284 acc: 0.49666666666666665\n",
            "New set of weights found, iteration: 70 loss: 0.95371175 acc: 0.37333333333333335\n",
            "New set of weights found, iteration: 74 loss: 0.94814533 acc: 0.39666666666666667\n",
            "New set of weights found, iteration: 76 loss: 0.93998027 acc: 0.6533333333333333\n",
            "New set of weights found, iteration: 80 loss: 0.9360327 acc: 0.49\n",
            "New set of weights found, iteration: 82 loss: 0.9209622 acc: 0.43333333333333335\n",
            "New set of weights found, iteration: 84 loss: 0.91947436 acc: 0.39666666666666667\n",
            "New set of weights found, iteration: 87 loss: 0.91914874 acc: 0.47\n",
            "New set of weights found, iteration: 88 loss: 0.91361684 acc: 0.3933333333333333\n",
            "New set of weights found, iteration: 92 loss: 0.9092631 acc: 0.3433333333333333\n",
            "New set of weights found, iteration: 93 loss: 0.88658935 acc: 0.37666666666666665\n",
            "New set of weights found, iteration: 95 loss: 0.8779051 acc: 0.38\n",
            "New set of weights found, iteration: 97 loss: 0.8683211 acc: 0.3933333333333333\n",
            "New set of weights found, iteration: 98 loss: 0.8658656 acc: 0.39666666666666667\n",
            "New set of weights found, iteration: 101 loss: 0.86440504 acc: 0.3566666666666667\n",
            "New set of weights found, iteration: 102 loss: 0.84767133 acc: 0.46\n",
            "New set of weights found, iteration: 106 loss: 0.8472598 acc: 0.43333333333333335\n",
            "New set of weights found, iteration: 108 loss: 0.8440553 acc: 0.3933333333333333\n",
            "New set of weights found, iteration: 113 loss: 0.8388296 acc: 0.5\n",
            "New set of weights found, iteration: 114 loss: 0.8205212 acc: 0.4066666666666667\n",
            "New set of weights found, iteration: 117 loss: 0.8181813 acc: 0.43\n",
            "New set of weights found, iteration: 125 loss: 0.81526875 acc: 0.49333333333333335\n",
            "New set of weights found, iteration: 128 loss: 0.807122 acc: 0.5\n",
            "New set of weights found, iteration: 130 loss: 0.8038813 acc: 0.6533333333333333\n",
            "New set of weights found, iteration: 132 loss: 0.8028078 acc: 0.7466666666666667\n",
            "New set of weights found, iteration: 139 loss: 0.7984704 acc: 0.74\n",
            "New set of weights found, iteration: 141 loss: 0.79498506 acc: 0.78\n",
            "New set of weights found, iteration: 142 loss: 0.7893583 acc: 0.8433333333333334\n",
            "New set of weights found, iteration: 143 loss: 0.7823634 acc: 0.7933333333333333\n",
            "New set of weights found, iteration: 144 loss: 0.77829343 acc: 0.79\n",
            "New set of weights found, iteration: 145 loss: 0.7615098 acc: 0.8466666666666667\n",
            "New set of weights found, iteration: 146 loss: 0.7559852 acc: 0.8233333333333334\n",
            "New set of weights found, iteration: 148 loss: 0.7546496 acc: 0.8866666666666667\n",
            "New set of weights found, iteration: 149 loss: 0.75092816 acc: 0.8966666666666666\n",
            "New set of weights found, iteration: 150 loss: 0.747195 acc: 0.84\n",
            "New set of weights found, iteration: 151 loss: 0.7432891 acc: 0.8066666666666666\n",
            "New set of weights found, iteration: 152 loss: 0.7340909 acc: 0.8666666666666667\n",
            "New set of weights found, iteration: 153 loss: 0.7306788 acc: 0.8166666666666667\n",
            "New set of weights found, iteration: 164 loss: 0.72321683 acc: 0.81\n",
            "New set of weights found, iteration: 165 loss: 0.72176784 acc: 0.81\n",
            "New set of weights found, iteration: 169 loss: 0.70957786 acc: 0.7866666666666666\n",
            "New set of weights found, iteration: 172 loss: 0.70772463 acc: 0.79\n",
            "New set of weights found, iteration: 177 loss: 0.70126116 acc: 0.82\n",
            "New set of weights found, iteration: 178 loss: 0.6999363 acc: 0.8666666666666667\n",
            "New set of weights found, iteration: 179 loss: 0.6988451 acc: 0.91\n",
            "New set of weights found, iteration: 191 loss: 0.68873036 acc: 0.85\n",
            "New set of weights found, iteration: 192 loss: 0.68831205 acc: 0.91\n",
            "New set of weights found, iteration: 193 loss: 0.67331904 acc: 0.8866666666666667\n",
            "New set of weights found, iteration: 198 loss: 0.66815454 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 204 loss: 0.66457903 acc: 0.9166666666666666\n",
            "New set of weights found, iteration: 206 loss: 0.6610892 acc: 0.8633333333333333\n",
            "New set of weights found, iteration: 207 loss: 0.6605678 acc: 0.8766666666666667\n",
            "New set of weights found, iteration: 209 loss: 0.6497643 acc: 0.8633333333333333\n",
            "New set of weights found, iteration: 217 loss: 0.64233035 acc: 0.84\n",
            "New set of weights found, iteration: 222 loss: 0.6386377 acc: 0.8366666666666667\n",
            "New set of weights found, iteration: 226 loss: 0.6359872 acc: 0.8966666666666666\n",
            "New set of weights found, iteration: 227 loss: 0.62573636 acc: 0.92\n",
            "New set of weights found, iteration: 230 loss: 0.62071383 acc: 0.94\n",
            "New set of weights found, iteration: 231 loss: 0.61711794 acc: 0.91\n",
            "New set of weights found, iteration: 232 loss: 0.61481416 acc: 0.8733333333333333\n",
            "New set of weights found, iteration: 233 loss: 0.6091018 acc: 0.8933333333333333\n",
            "New set of weights found, iteration: 234 loss: 0.6086884 acc: 0.9133333333333333\n",
            "New set of weights found, iteration: 236 loss: 0.607606 acc: 0.84\n",
            "New set of weights found, iteration: 242 loss: 0.60711265 acc: 0.81\n",
            "New set of weights found, iteration: 243 loss: 0.6068115 acc: 0.89\n",
            "New set of weights found, iteration: 245 loss: 0.59996223 acc: 0.8233333333333334\n",
            "New set of weights found, iteration: 247 loss: 0.58067167 acc: 0.76\n",
            "New set of weights found, iteration: 249 loss: 0.5784168 acc: 0.83\n",
            "New set of weights found, iteration: 253 loss: 0.57657325 acc: 0.7966666666666666\n",
            "New set of weights found, iteration: 255 loss: 0.574722 acc: 0.8266666666666667\n",
            "New set of weights found, iteration: 258 loss: 0.57312167 acc: 0.8133333333333334\n",
            "New set of weights found, iteration: 261 loss: 0.56925696 acc: 0.7933333333333333\n",
            "New set of weights found, iteration: 265 loss: 0.56074023 acc: 0.7966666666666666\n",
            "New set of weights found, iteration: 267 loss: 0.5605342 acc: 0.7866666666666666\n",
            "New set of weights found, iteration: 268 loss: 0.55364096 acc: 0.8233333333333334\n",
            "New set of weights found, iteration: 269 loss: 0.55124503 acc: 0.8233333333333334\n",
            "New set of weights found, iteration: 275 loss: 0.5425308 acc: 0.86\n",
            "New set of weights found, iteration: 281 loss: 0.5386393 acc: 0.8833333333333333\n",
            "New set of weights found, iteration: 284 loss: 0.5336918 acc: 0.9133333333333333\n",
            "New set of weights found, iteration: 289 loss: 0.5332634 acc: 0.93\n",
            "New set of weights found, iteration: 295 loss: 0.52366376 acc: 0.87\n",
            "New set of weights found, iteration: 297 loss: 0.5215278 acc: 0.9033333333333333\n",
            "New set of weights found, iteration: 298 loss: 0.5201153 acc: 0.9066666666666666\n",
            "New set of weights found, iteration: 302 loss: 0.512227 acc: 0.9166666666666666\n",
            "New set of weights found, iteration: 303 loss: 0.5120188 acc: 0.89\n",
            "New set of weights found, iteration: 311 loss: 0.4982469 acc: 0.86\n",
            "New set of weights found, iteration: 314 loss: 0.49593496 acc: 0.8866666666666667\n",
            "New set of weights found, iteration: 318 loss: 0.49163595 acc: 0.9133333333333333\n",
            "New set of weights found, iteration: 320 loss: 0.48986998 acc: 0.8566666666666667\n",
            "New set of weights found, iteration: 323 loss: 0.48643026 acc: 0.84\n",
            "New set of weights found, iteration: 327 loss: 0.47830313 acc: 0.8766666666666667\n",
            "New set of weights found, iteration: 328 loss: 0.47417724 acc: 0.8633333333333333\n",
            "New set of weights found, iteration: 333 loss: 0.47327647 acc: 0.8466666666666667\n",
            "New set of weights found, iteration: 335 loss: 0.46838614 acc: 0.8533333333333334\n",
            "New set of weights found, iteration: 338 loss: 0.46647513 acc: 0.8166666666666667\n",
            "New set of weights found, iteration: 352 loss: 0.46608245 acc: 0.8333333333333334\n",
            "New set of weights found, iteration: 353 loss: 0.46205565 acc: 0.8333333333333334\n",
            "New set of weights found, iteration: 355 loss: 0.46179625 acc: 0.8666666666666667\n",
            "New set of weights found, iteration: 356 loss: 0.45656422 acc: 0.91\n",
            "New set of weights found, iteration: 357 loss: 0.44896582 acc: 0.8833333333333333\n",
            "New set of weights found, iteration: 358 loss: 0.44879395 acc: 0.8966666666666666\n",
            "New set of weights found, iteration: 362 loss: 0.44568396 acc: 0.9033333333333333\n",
            "New set of weights found, iteration: 364 loss: 0.44045186 acc: 0.9033333333333333\n",
            "New set of weights found, iteration: 365 loss: 0.43207875 acc: 0.8833333333333333\n",
            "New set of weights found, iteration: 367 loss: 0.4316861 acc: 0.8866666666666667\n",
            "New set of weights found, iteration: 373 loss: 0.42247173 acc: 0.9\n",
            "New set of weights found, iteration: 378 loss: 0.41963083 acc: 0.8933333333333333\n",
            "New set of weights found, iteration: 391 loss: 0.41803762 acc: 0.9166666666666666\n",
            "New set of weights found, iteration: 394 loss: 0.40722442 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 399 loss: 0.40406913 acc: 0.8966666666666666\n",
            "New set of weights found, iteration: 402 loss: 0.40190095 acc: 0.92\n",
            "New set of weights found, iteration: 403 loss: 0.39493388 acc: 0.91\n",
            "New set of weights found, iteration: 405 loss: 0.3946071 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 407 loss: 0.39355513 acc: 0.91\n",
            "New set of weights found, iteration: 408 loss: 0.39139694 acc: 0.9166666666666666\n",
            "New set of weights found, iteration: 411 loss: 0.38513803 acc: 0.9166666666666666\n",
            "New set of weights found, iteration: 415 loss: 0.3851139 acc: 0.9033333333333333\n",
            "New set of weights found, iteration: 417 loss: 0.38471276 acc: 0.92\n",
            "New set of weights found, iteration: 419 loss: 0.37787083 acc: 0.9166666666666666\n",
            "New set of weights found, iteration: 422 loss: 0.37112856 acc: 0.93\n",
            "New set of weights found, iteration: 429 loss: 0.36890668 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 431 loss: 0.36537284 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 432 loss: 0.3608864 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 451 loss: 0.3608689 acc: 0.93\n",
            "New set of weights found, iteration: 455 loss: 0.35670125 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 461 loss: 0.3564989 acc: 0.9166666666666666\n",
            "New set of weights found, iteration: 467 loss: 0.35490963 acc: 0.91\n",
            "New set of weights found, iteration: 468 loss: 0.34977043 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 469 loss: 0.3478622 acc: 0.9166666666666666\n",
            "New set of weights found, iteration: 472 loss: 0.34403688 acc: 0.9166666666666666\n",
            "New set of weights found, iteration: 473 loss: 0.33940262 acc: 0.92\n",
            "New set of weights found, iteration: 475 loss: 0.3326536 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 478 loss: 0.33011964 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 480 loss: 0.33003053 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 482 loss: 0.32306594 acc: 0.9266666666666666\n",
            "New set of weights found, iteration: 484 loss: 0.3219561 acc: 0.92\n",
            "New set of weights found, iteration: 485 loss: 0.3159662 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 486 loss: 0.31526706 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 488 loss: 0.3144667 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 493 loss: 0.3126857 acc: 0.93\n",
            "New set of weights found, iteration: 500 loss: 0.3105838 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 501 loss: 0.3088618 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 502 loss: 0.30151534 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 503 loss: 0.29959518 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 506 loss: 0.29714733 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 508 loss: 0.29615584 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 513 loss: 0.29281542 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 517 loss: 0.2920112 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 519 loss: 0.28795934 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 527 loss: 0.28627732 acc: 0.94\n",
            "New set of weights found, iteration: 528 loss: 0.28597823 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 544 loss: 0.28490874 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 557 loss: 0.28433517 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 558 loss: 0.2828499 acc: 0.94\n",
            "New set of weights found, iteration: 561 loss: 0.28269076 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 566 loss: 0.28115714 acc: 0.94\n",
            "New set of weights found, iteration: 568 loss: 0.27919886 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 570 loss: 0.27495274 acc: 0.94\n",
            "New set of weights found, iteration: 574 loss: 0.2727014 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 609 loss: 0.2714655 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 611 loss: 0.2713919 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 612 loss: 0.27035764 acc: 0.94\n",
            "New set of weights found, iteration: 614 loss: 0.26474217 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 616 loss: 0.26278028 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 618 loss: 0.26183742 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 625 loss: 0.2605882 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 630 loss: 0.2588303 acc: 0.94\n",
            "New set of weights found, iteration: 637 loss: 0.25712305 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 639 loss: 0.25669003 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 641 loss: 0.25601378 acc: 0.92\n",
            "New set of weights found, iteration: 648 loss: 0.25601125 acc: 0.9166666666666666\n",
            "New set of weights found, iteration: 649 loss: 0.25202808 acc: 0.9233333333333333\n",
            "New set of weights found, iteration: 651 loss: 0.2502158 acc: 0.9233333333333333\n",
            "New set of weights found, iteration: 652 loss: 0.24661733 acc: 0.94\n",
            "New set of weights found, iteration: 653 loss: 0.24651732 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 654 loss: 0.24549128 acc: 0.94\n",
            "New set of weights found, iteration: 657 loss: 0.24192917 acc: 0.94\n",
            "New set of weights found, iteration: 658 loss: 0.24181387 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 676 loss: 0.24085978 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 679 loss: 0.23738421 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 687 loss: 0.23695572 acc: 0.94\n",
            "New set of weights found, iteration: 703 loss: 0.23367447 acc: 0.94\n",
            "New set of weights found, iteration: 705 loss: 0.23038976 acc: 0.94\n",
            "New set of weights found, iteration: 715 loss: 0.22943924 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 718 loss: 0.22921188 acc: 0.94\n",
            "New set of weights found, iteration: 722 loss: 0.22903605 acc: 0.94\n",
            "New set of weights found, iteration: 729 loss: 0.22437958 acc: 0.94\n",
            "New set of weights found, iteration: 734 loss: 0.22339946 acc: 0.94\n",
            "New set of weights found, iteration: 748 loss: 0.22166005 acc: 0.94\n",
            "New set of weights found, iteration: 760 loss: 0.2189123 acc: 0.94\n",
            "New set of weights found, iteration: 761 loss: 0.21827835 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 770 loss: 0.21546504 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 772 loss: 0.21400948 acc: 0.94\n",
            "New set of weights found, iteration: 778 loss: 0.21398969 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 779 loss: 0.21334386 acc: 0.94\n",
            "New set of weights found, iteration: 782 loss: 0.21272907 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 787 loss: 0.21029286 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 797 loss: 0.2067276 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 802 loss: 0.20448728 acc: 0.94\n",
            "New set of weights found, iteration: 805 loss: 0.2018003 acc: 0.94\n",
            "New set of weights found, iteration: 810 loss: 0.20163727 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 813 loss: 0.19924417 acc: 0.94\n",
            "New set of weights found, iteration: 820 loss: 0.19734994 acc: 0.94\n",
            "New set of weights found, iteration: 821 loss: 0.19692986 acc: 0.94\n",
            "New set of weights found, iteration: 828 loss: 0.19359593 acc: 0.94\n",
            "New set of weights found, iteration: 832 loss: 0.19234332 acc: 0.94\n",
            "New set of weights found, iteration: 844 loss: 0.19068971 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 845 loss: 0.18869092 acc: 0.94\n",
            "New set of weights found, iteration: 847 loss: 0.1871506 acc: 0.9533333333333334\n",
            "New set of weights found, iteration: 849 loss: 0.18669562 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 850 loss: 0.18417157 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 851 loss: 0.18264878 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 856 loss: 0.18215491 acc: 0.94\n",
            "New set of weights found, iteration: 858 loss: 0.17922138 acc: 0.94\n",
            "New set of weights found, iteration: 868 loss: 0.17918922 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 874 loss: 0.17882352 acc: 0.94\n",
            "New set of weights found, iteration: 886 loss: 0.17795885 acc: 0.94\n",
            "New set of weights found, iteration: 890 loss: 0.17705464 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 891 loss: 0.17530137 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 899 loss: 0.17306212 acc: 0.94\n",
            "New set of weights found, iteration: 910 loss: 0.17219882 acc: 0.94\n",
            "New set of weights found, iteration: 923 loss: 0.17013294 acc: 0.94\n",
            "New set of weights found, iteration: 933 loss: 0.16966693 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 937 loss: 0.1686811 acc: 0.94\n",
            "New set of weights found, iteration: 941 loss: 0.16746922 acc: 0.94\n",
            "New set of weights found, iteration: 942 loss: 0.16645212 acc: 0.94\n",
            "New set of weights found, iteration: 960 loss: 0.16618024 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 971 loss: 0.16574873 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 980 loss: 0.16406298 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 987 loss: 0.16362067 acc: 0.94\n",
            "New set of weights found, iteration: 999 loss: 0.16293563 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 1032 loss: 0.16220799 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1077 loss: 0.16196755 acc: 0.94\n",
            "New set of weights found, iteration: 1090 loss: 0.16120633 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1097 loss: 0.16036181 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 1101 loss: 0.1603522 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 1123 loss: 0.16029394 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1129 loss: 0.15903012 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 1133 loss: 0.15832289 acc: 0.94\n",
            "New set of weights found, iteration: 1142 loss: 0.15815979 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 1144 loss: 0.15706678 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1147 loss: 0.15704954 acc: 0.94\n",
            "New set of weights found, iteration: 1153 loss: 0.15652262 acc: 0.94\n",
            "New set of weights found, iteration: 1158 loss: 0.15553713 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 1183 loss: 0.15551537 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 1186 loss: 0.15512952 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1237 loss: 0.15492696 acc: 0.94\n",
            "New set of weights found, iteration: 1265 loss: 0.15475857 acc: 0.94\n",
            "New set of weights found, iteration: 1269 loss: 0.15307455 acc: 0.94\n",
            "New set of weights found, iteration: 1284 loss: 0.15298109 acc: 0.94\n",
            "New set of weights found, iteration: 1286 loss: 0.15233037 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1293 loss: 0.15190287 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1297 loss: 0.15093216 acc: 0.94\n",
            "New set of weights found, iteration: 1318 loss: 0.15089014 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1323 loss: 0.15085807 acc: 0.94\n",
            "New set of weights found, iteration: 1325 loss: 0.15074143 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 1326 loss: 0.15042466 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 1337 loss: 0.14978988 acc: 0.94\n",
            "New set of weights found, iteration: 1358 loss: 0.14976592 acc: 0.94\n",
            "New set of weights found, iteration: 1362 loss: 0.14890413 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1370 loss: 0.14890376 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1373 loss: 0.14879283 acc: 0.94\n",
            "New set of weights found, iteration: 1380 loss: 0.14847784 acc: 0.94\n",
            "New set of weights found, iteration: 1386 loss: 0.14847465 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1388 loss: 0.14800072 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1391 loss: 0.14678228 acc: 0.94\n",
            "New set of weights found, iteration: 1408 loss: 0.1464346 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1409 loss: 0.14527605 acc: 0.9333333333333333\n",
            "New set of weights found, iteration: 1411 loss: 0.14525828 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1430 loss: 0.1446538 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1434 loss: 0.14401802 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 1489 loss: 0.14292544 acc: 0.94\n",
            "New set of weights found, iteration: 1495 loss: 0.14265014 acc: 0.94\n",
            "New set of weights found, iteration: 1505 loss: 0.14241044 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 1511 loss: 0.14230332 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 1513 loss: 0.14213061 acc: 0.94\n",
            "New set of weights found, iteration: 1540 loss: 0.14207764 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 1543 loss: 0.1418695 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1549 loss: 0.14167489 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 1555 loss: 0.14166912 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 1562 loss: 0.14150883 acc: 0.94\n",
            "New set of weights found, iteration: 1633 loss: 0.14122452 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1641 loss: 0.14051384 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1644 loss: 0.14047976 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1653 loss: 0.14024459 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1672 loss: 0.14020847 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 1682 loss: 0.13941014 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1689 loss: 0.13929491 acc: 0.9466666666666667\n",
            "New set of weights found, iteration: 1693 loss: 0.1389391 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1696 loss: 0.13860264 acc: 0.94\n",
            "New set of weights found, iteration: 1721 loss: 0.13797918 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 1754 loss: 0.13770229 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 1755 loss: 0.13764334 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1774 loss: 0.13745008 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1777 loss: 0.13731971 acc: 0.94\n",
            "New set of weights found, iteration: 1792 loss: 0.13701116 acc: 0.94\n",
            "New set of weights found, iteration: 1795 loss: 0.13694005 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1832 loss: 0.13684916 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1870 loss: 0.1364587 acc: 0.94\n",
            "New set of weights found, iteration: 1917 loss: 0.13611524 acc: 0.94\n",
            "New set of weights found, iteration: 1964 loss: 0.1361099 acc: 0.94\n",
            "New set of weights found, iteration: 1974 loss: 0.13602664 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 1994 loss: 0.1358595 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 2006 loss: 0.13569748 acc: 0.94\n",
            "New set of weights found, iteration: 2097 loss: 0.135575 acc: 0.94\n",
            "New set of weights found, iteration: 2196 loss: 0.13544048 acc: 0.94\n",
            "New set of weights found, iteration: 2216 loss: 0.13506924 acc: 0.94\n",
            "New set of weights found, iteration: 2330 loss: 0.13501963 acc: 0.94\n",
            "New set of weights found, iteration: 2349 loss: 0.1349512 acc: 0.94\n",
            "New set of weights found, iteration: 2406 loss: 0.13495022 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 2418 loss: 0.13487558 acc: 0.94\n",
            "New set of weights found, iteration: 2419 loss: 0.13467331 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 2626 loss: 0.13460639 acc: 0.94\n",
            "New set of weights found, iteration: 2635 loss: 0.13460459 acc: 0.94\n",
            "New set of weights found, iteration: 2658 loss: 0.13457163 acc: 0.94\n",
            "New set of weights found, iteration: 2697 loss: 0.134482 acc: 0.94\n",
            "New set of weights found, iteration: 2706 loss: 0.13440925 acc: 0.94\n",
            "New set of weights found, iteration: 2802 loss: 0.13435768 acc: 0.94\n",
            "New set of weights found, iteration: 3718 loss: 0.13433526 acc: 0.94\n",
            "New set of weights found, iteration: 3728 loss: 0.1341544 acc: 0.94\n",
            "New set of weights found, iteration: 3944 loss: 0.13412814 acc: 0.94\n",
            "New set of weights found, iteration: 4289 loss: 0.1339631 acc: 0.94\n",
            "New set of weights found, iteration: 4358 loss: 0.13394862 acc: 0.94\n",
            "New set of weights found, iteration: 4742 loss: 0.13389273 acc: 0.94\n",
            "New set of weights found, iteration: 4760 loss: 0.13381386 acc: 0.94\n",
            "New set of weights found, iteration: 4788 loss: 0.133766 acc: 0.94\n",
            "New set of weights found, iteration: 4999 loss: 0.13366133 acc: 0.94\n",
            "New set of weights found, iteration: 5083 loss: 0.13359268 acc: 0.94\n",
            "New set of weights found, iteration: 5401 loss: 0.13353345 acc: 0.94\n",
            "New set of weights found, iteration: 5683 loss: 0.1335304 acc: 0.94\n",
            "New set of weights found, iteration: 5698 loss: 0.13346647 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 5724 loss: 0.13338915 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 5743 loss: 0.13331929 acc: 0.94\n",
            "New set of weights found, iteration: 5802 loss: 0.13327837 acc: 0.94\n",
            "New set of weights found, iteration: 5925 loss: 0.13315395 acc: 0.94\n",
            "New set of weights found, iteration: 5988 loss: 0.1329223 acc: 0.94\n",
            "New set of weights found, iteration: 6294 loss: 0.13287486 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 6301 loss: 0.13271876 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 6910 loss: 0.132702 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 7205 loss: 0.13261841 acc: 0.94\n",
            "New set of weights found, iteration: 7226 loss: 0.1325952 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 7330 loss: 0.13258275 acc: 0.94\n",
            "New set of weights found, iteration: 7347 loss: 0.13236 acc: 0.94\n",
            "New set of weights found, iteration: 7623 loss: 0.13233615 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 7825 loss: 0.13225725 acc: 0.94\n",
            "New set of weights found, iteration: 7980 loss: 0.13218153 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 8185 loss: 0.13214643 acc: 0.94\n",
            "New set of weights found, iteration: 8397 loss: 0.13208903 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 8489 loss: 0.13195354 acc: 0.94\n",
            "New set of weights found, iteration: 8836 loss: 0.13188124 acc: 0.9366666666666666\n",
            "New set of weights found, iteration: 8944 loss: 0.13186565 acc: 0.94\n",
            "New set of weights found, iteration: 9022 loss: 0.13179782 acc: 0.9433333333333334\n",
            "New set of weights found, iteration: 9053 loss: 0.13167004 acc: 0.9433333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44O1hsiSYHOb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvMkYEduYK8R"
      },
      "source": [
        "**Backpropagation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BaiU9KjIf_1"
      },
      "source": [
        "# Dense layer\n",
        "class Layer_Dense :\n",
        "  # Layer initialization\n",
        "  def __init__ ( self , n_inputs , n_neurons ):\n",
        "    # Initialize weights and biases\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros(( 1 , n_neurons))\n",
        "    # Forward pass\n",
        "\n",
        "  def forward ( self , inputs ):\n",
        "    self.inputs = inputs\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "  \n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues ):\n",
        "    # Gradients on parameters\n",
        "    self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "    self.dbiases = np.sum(dvalues, axis = 0 , keepdims = True )\n",
        "    \n",
        "    # Gradient on values\n",
        "    self.dinputs = np.dot(dvalues, self.weights.T)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gnxrCSBZpuY"
      },
      "source": [
        "# ReLU activation\n",
        "class Activation_ReLU :\n",
        "  # Forward pass\n",
        "  def forward( self , inputs ):\n",
        "    # Remember input values\n",
        "    self.inputs = inputs\n",
        "    # Calculate output values from inputs\n",
        "    self.output = np.maximum( 0 , inputs)\n",
        "\n",
        "  # Backward pass\n",
        "  def backward( self , dvalues ):\n",
        "    # Since we need to modify the original variable,\n",
        "    # let's make a copy of the values first\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Zero gradient where input values were negative\n",
        "    self.dinputs[self.inputs <= 0 ] = 0"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT9fyCwqaR_K"
      },
      "source": [
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy ( Loss ):\n",
        "  # Forward pass\n",
        "  def forward( self , y_pred , y_true ):\n",
        "    # Number of samples in a batch\n",
        "    samples = len(y_pred)\n",
        "\n",
        "    # Clip data to prevent division by 0\n",
        "    # Clip both sides to not drag mean towards any value\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7 , 1 - 1e-7 )\n",
        "\n",
        "    # Probabilities for target values -\n",
        "    # only if categorical labels\n",
        "    if len(y_true.shape) == 1 :\n",
        "      correct_confidences = y_pred_clipped[range(samples),y_true]\n",
        "    elif len(y_true.shape) == 2 : # Mask values - only for one-hot encoded labels\n",
        "      correct_confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
        "\n",
        "    # Losses\n",
        "    negative_log_likelihoods = - np.log(correct_confidences)\n",
        "\n",
        "    return negative_log_likelihoods\n",
        "\n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues , y_true ):\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "\n",
        "    # Number of labels in every sample\n",
        "    # We'll use the first sample to count them\n",
        "    labels = len(dvalues[ 0 ])\n",
        "\n",
        "    # If labels are sparse, turn them into one-hot vector\n",
        "    if len(y_true.shape) == 1 :\n",
        "      y_true = np.eye(labels)[y_true]\n",
        "\n",
        "    # Calculate gradient\n",
        "    self.dinputs = - y_true / dvalues\n",
        "\n",
        "    # Normalize gradient\n",
        "    self.dinputs = self.dinputs / samples"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWOX-Wi6dBoZ"
      },
      "source": [
        "# Softmax activation\n",
        "class Activation_Softmax :\n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Get unnormalized probabilities\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis = 1 , keepdims = True ))\n",
        "\n",
        "    # Normalize them for each sample\n",
        "    probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True )\n",
        "    \n",
        "    self.output = probabilities\n",
        "\n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues ):\n",
        "    # Create uninitialized array\n",
        "    self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "    # Enumerate outputs and gradients\n",
        "    for index, (single_output, single_dvalues) in enumerate ( zip (self.output, dvalues)):\n",
        "      # Flatten output array\n",
        "      single_output = single_output.reshape( - 1 , 1 )\n",
        "\n",
        "      # Calculate Jacobian matrix of the output and\n",
        "      jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "\n",
        "      # Calculate sample-wise gradient\n",
        "      # and add it to the array of sample gradients\n",
        "      self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfhrYsPMdDOU"
      },
      "source": [
        "# Softmax classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy ():\n",
        "  # Creates activation and loss function objects\n",
        "  def __init__( self ):\n",
        "    self.activation = Activation_Softmax()\n",
        "    self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "  # Forward pass\n",
        "  def forward( self , inputs , y_true ):\n",
        "    # Output layer's activation function\n",
        "    self.activation.forward(inputs)\n",
        "    # Set the output\n",
        "    self.output = self.activation.output\n",
        "    # Calculate and return loss value\n",
        "    return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "  # Backward pass\n",
        "  def backward( self , dvalues , y_true ):\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "\n",
        "    # If labels are one-hot encoded,\n",
        "    # turn them into discrete values\n",
        "    if len(y_true.shape) == 2 :\n",
        "      y_true = np.argmax(y_true, axis = 1)\n",
        "\n",
        "    # Copy so we can safely modify\n",
        "    self.dinputs = dvalues.copy()\n",
        "\n",
        "    # Calculate gradient\n",
        "    self.dinputs[ range (samples), y_true] -= 1\n",
        "\n",
        "    # Normalize gradient\n",
        "    self.dinputs = self.dinputs / samples"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFE6-zlnjrMc"
      },
      "source": [
        "Test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvTbelFMjS67"
      },
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "\n",
        "nnfs.init()\n",
        "\n",
        "softmax_outputs = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
        "                            [ 0.1 , 0.5 , 0.4 ],\n",
        "                            [ 0.02 , 0.9 , 0.08 ]])\n",
        "\n",
        "class_targets = np.array([ 0 , 1 , 1 ])"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJeu2f3jj-ut"
      },
      "source": [
        "\n",
        "softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "softmax_loss.backward(softmax_outputs, class_targets)\n",
        "\n",
        "dvalues1 = softmax_loss.dinputs\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkPxQdMvlEoP"
      },
      "source": [
        "\n",
        "activation = Activation_Softmax()\n",
        "\n",
        "activation.output = softmax_outputs\n",
        "\n",
        "loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "loss.backward(softmax_outputs, class_targets)\n",
        "\n",
        "activation.backward(loss.dinputs)\n",
        "\n",
        "dvalues2 = activation.dinputs\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSELdPMglGKL",
        "outputId": "a0738116-3674-4726-8179-b6fdf7ee9b4d"
      },
      "source": [
        "print ( 'Gradients: combined loss and activation:' )\n",
        "print (dvalues1)\n",
        "print ( 'Gradients: separate loss and activation:' )\n",
        "print (dvalues2)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradients: combined loss and activation:\n",
            "[[-0.1         0.03333333  0.06666667]\n",
            " [ 0.03333333 -0.16666667  0.13333333]\n",
            " [ 0.00666667 -0.03333333  0.02666667]]\n",
            "Gradients: separate loss and activation:\n",
            "[[-0.09999999  0.03333334  0.06666667]\n",
            " [ 0.03333334 -0.16666667  0.13333334]\n",
            " [ 0.00666667 -0.03333333  0.02666667]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv1WAl7GljMS"
      },
      "source": [
        "Full model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3IMJe_jlGus",
        "outputId": "2b1a494b-d827-4a25-8b76-3e93827cbe76"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data( samples = 100 , classes = 3 )\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Layer_Dense( 2 , 3 )\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 3 input features (as we take output\n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense( 3 , 3 )\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Perform a forward pass of our training data through this layer\n",
        "dense1.forward(X)\n",
        "\n",
        "# Perform a forward pass through activation function\n",
        "# takes the output of first dense layer here\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "# Perform a forward pass through second Dense layer\n",
        "# takes outputs of activation function of first layer as inputs\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "# Perform a forward pass through the activation/loss function\n",
        "# takes the output of second dense layer here and returns loss\n",
        "loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "# Let's see output of the first few samples:\n",
        "print (loss_activation.output[: 5 ])\n",
        "\n",
        "# Print loss value\n",
        "print ( 'loss:' , loss)\n",
        "\n",
        "# Calculate accuracy from output of activation2 and targets\n",
        "# calculate values along first axis\n",
        "predictions = np.argmax(loss_activation.output, axis = 1 )\n",
        "\n",
        "if len (y.shape) == 2 :\n",
        "  y = np.argmax(y, axis = 1 )\n",
        "\n",
        "accuracy = np.mean(predictions == y)\n",
        "\n",
        "# Print accuracy\n",
        "print ( 'acc:' , accuracy)\n",
        "\n",
        "# Backward pass\n",
        "loss_activation.backward(loss_activation.output, y)\n",
        "dense2.backward(loss_activation.dinputs)\n",
        "activation1.backward(dense2.dinputs)\n",
        "dense1.backward(activation1.dinputs)\n",
        "\n",
        "# Print gradients\n",
        "print (dense1.dweights)\n",
        "print (dense1.dbiases)\n",
        "print (dense2.dweights)\n",
        "print (dense2.dbiases)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.3333332  0.3333332  0.33333364]\n",
            " [0.3333329  0.33333293 0.3333342 ]\n",
            " [0.3333326  0.33333263 0.33333477]\n",
            " [0.33333233 0.3333324  0.33333528]]\n",
            "loss: 1.0986104\n",
            "acc: 0.34\n",
            "[[ 1.5766357e-04  7.8368583e-05  4.7324400e-05]\n",
            " [ 1.8161038e-04  1.1045573e-05 -3.3096312e-05]]\n",
            "[[-3.60553473e-04  9.66117223e-05 -1.03671395e-04]]\n",
            "[[ 5.44109462e-05  1.07411419e-04 -1.61822361e-04]\n",
            " [-4.07913431e-05 -7.16780924e-05  1.12469446e-04]\n",
            " [-5.30112993e-05  8.58172934e-05 -3.28059905e-05]]\n",
            "[[-1.0729185e-05 -9.4610732e-06  2.0027859e-05]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJSNsxCqmjnq"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWehkppsw4Lj"
      },
      "source": [
        "**Optimizers**\n",
        "<br>\n",
        "*Stochastic Gradient Descent*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYedrUdwl8wr"
      },
      "source": [
        "class Optimizer_SGD :\n",
        "  # Initialize optimizer - set settings,\n",
        "  # learning rate of 1. is default for this optimizer\n",
        "  def __init__( self , learning_rate = 1.0 ):\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "  # Update parameters\n",
        "  def update_params( self , layer ):\n",
        "    layer.weights += - self.learning_rate * layer.dweights\n",
        "    layer.biases += - self.learning_rate * layer.dbiases"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESD4P2ThxJtn"
      },
      "source": [
        "optimizer = Optimizer_SGD()\n",
        "optimizer.update_params(dense1)\n",
        "optimizer.update_params(dense2)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5J6ayAuyIy_"
      },
      "source": [
        "Let’s make a 1x64 densely-connected neural network (1\n",
        "hidden layer with 64 neurons)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjhYEopGxPdK"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data( samples = 100 , classes = 3 )\n",
        "\n",
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense( 2 , 64 )\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 64 input features (as we take output\n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense( 64 , 3 )\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8zRAbuByZx1"
      },
      "source": [
        "The next step is to create the optimizer’s object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqsezPfByVw2"
      },
      "source": [
        "# Create optimizer\n",
        "optimizer = Optimizer_SGD()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFICCg_GycZK"
      },
      "source": [
        "Then perform a forward pass of our sample data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVfiBfnZyXzh",
        "outputId": "07bbca40-5ce5-4243-da17-cd66562a419c"
      },
      "source": [
        "# Perform a forward pass of our training data through this layer\n",
        "dense1.forward(X)\n",
        "\n",
        "# Perform a forward pass through activation function\n",
        "# takes the output of first dense layer here\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "# Perform a forward pass through second Dense layer\n",
        "# takes outputs of activation function of first layer as inputs\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "# Perform a forward pass through the activation/loss function\n",
        "# takes the output of second dense layer here and returns loss\n",
        "loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "# Let's print loss value\n",
        "print ( 'loss:' , loss)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 1.0986058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVXAIsOWykPC",
        "outputId": "068045a9-3d6f-4d26-9630-3eacc434102c"
      },
      "source": [
        "# Calculate accuracy from output of activation2 and targets\n",
        "# calculate values along first axis\n",
        "predictions = np.argmax(loss_activation.output, axis = 1 )\n",
        "\n",
        "if len (y.shape) == 2 :\n",
        "  y = np.argmax(y, axis = 1)\n",
        "\n",
        "accuracy = np.mean(predictions == y)\n",
        "\n",
        "print ( 'acc:' , accuracy)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc: 0.3466666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM9LzWusyvm-"
      },
      "source": [
        "Next, we do our backward pass , which is also called backpropagation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYze6oZRyoL_"
      },
      "source": [
        "# Backward pass\n",
        "loss_activation.backward(loss_activation.output, y)\n",
        "\n",
        "dense2.backward(loss_activation.dinputs)\n",
        "\n",
        "activation1.backward(dense2.dinputs)\n",
        "\n",
        "dense1.backward(activation1.dinputs)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axjufetcy4wX"
      },
      "source": [
        "Then we finally use our optimizer to update weights and biases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdKwbMPFy1FA"
      },
      "source": [
        "# Update weights and biases\n",
        "optimizer.update_params(dense1)\n",
        "optimizer.update_params(dense2)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTD_QPbszYE7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHui3IYCzaQj"
      },
      "source": [
        "Perform mulitple epochs to train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmPWJ3key7Gt"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data( samples = 100 , classes = 3 )\n",
        "\n",
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense( 2 , 64 )\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 64 input features (as we take output\n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense( 64 , 3 )\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_SGD()"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIiM-_VdzhcQ",
        "outputId": "038a94d2-2c62-4ea9-ff69-c16d86d391f8"
      },
      "source": [
        "# Train in loop\n",
        "for epoch in range ( 10001 ):\n",
        "  # Perform a forward pass of our training data through this layer\n",
        "  dense1.forward(X)\n",
        "\n",
        "  # Perform a forward pass through activation function\n",
        "  # takes the output of first dense layer here\n",
        "  activation1.forward(dense1.output)\n",
        "\n",
        "  # Perform a forward pass through second Dense layer\n",
        "  # takes outputs of activation function of first layer as inputs\n",
        "  dense2.forward(activation1.output)\n",
        "\n",
        "  # Perform a forward pass through the activation/loss function\n",
        "  # takes the output of second dense layer here and returns loss\n",
        "  loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "  # Calculate accuracy from output of activation2 and targets\n",
        "  # calculate values along first axis\n",
        "  predictions = np.argmax(loss_activation.output, axis = 1 )\n",
        "\n",
        "  if len (y.shape) == 2 :\n",
        "    y = np.argmax(y, axis = 1 )\n",
        "\n",
        "  accuracy = np.mean(predictions == y)\n",
        "\n",
        "  if not epoch % 100 :\n",
        "    print(\"epoch: \" + str(epoch) + ', '+ \"acc: \" + str(accuracy) + \", loss: \" + str(loss))\n",
        "\n",
        "  # Backward pass\n",
        "  loss_activation.backward(loss_activation.output, y)\n",
        "\n",
        "  dense2.backward(loss_activation.dinputs)\n",
        "\n",
        "  activation1.backward(dense2.dinputs)\n",
        "\n",
        "  dense1.backward(activation1.dinputs)\n",
        "\n",
        "  # Update weights and biases\n",
        "  optimizer.update_params(dense1)\n",
        "  optimizer.update_params(dense2)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.7433333333333333, loss: 0.5842397\n",
            "epoch: 100, acc: 0.7566666666666667, loss: 0.56305933\n",
            "epoch: 200, acc: 0.7033333333333334, loss: 0.5941446\n",
            "epoch: 300, acc: 0.74, loss: 0.58630407\n",
            "epoch: 400, acc: 0.7466666666666667, loss: 0.58915913\n",
            "epoch: 500, acc: 0.7366666666666667, loss: 0.5824549\n",
            "epoch: 600, acc: 0.7366666666666667, loss: 0.5517462\n",
            "epoch: 700, acc: 0.74, loss: 0.5170456\n",
            "epoch: 800, acc: 0.73, loss: 0.5631136\n",
            "epoch: 900, acc: 0.74, loss: 0.577861\n",
            "epoch: 1000, acc: 0.7233333333333334, loss: 0.62130857\n",
            "epoch: 1100, acc: 0.72, loss: 0.6148063\n",
            "epoch: 1200, acc: 0.75, loss: 0.53872705\n",
            "epoch: 1300, acc: 0.6066666666666667, loss: 0.99692625\n",
            "epoch: 1400, acc: 0.65, loss: 0.8081043\n",
            "epoch: 1500, acc: 0.7066666666666667, loss: 0.6209616\n",
            "epoch: 1600, acc: 0.73, loss: 0.5498641\n",
            "epoch: 1700, acc: 0.7333333333333333, loss: 0.5676033\n",
            "epoch: 1800, acc: 0.76, loss: 0.5174197\n",
            "epoch: 1900, acc: 0.7466666666666667, loss: 0.55740833\n",
            "epoch: 2000, acc: 0.7366666666666667, loss: 0.5399109\n",
            "epoch: 2100, acc: 0.74, loss: 0.5596201\n",
            "epoch: 2200, acc: 0.7733333333333333, loss: 0.5248826\n",
            "epoch: 2300, acc: 0.7866666666666666, loss: 0.47814092\n",
            "epoch: 2400, acc: 0.74, loss: 0.55771583\n",
            "epoch: 2500, acc: 0.74, loss: 0.53421235\n",
            "epoch: 2600, acc: 0.7433333333333333, loss: 0.5536757\n",
            "epoch: 2700, acc: 0.7433333333333333, loss: 0.5586399\n",
            "epoch: 2800, acc: 0.7433333333333333, loss: 0.55974245\n",
            "epoch: 2900, acc: 0.74, loss: 0.54525834\n",
            "epoch: 3000, acc: 0.75, loss: 0.5380724\n",
            "epoch: 3100, acc: 0.73, loss: 0.58097035\n",
            "epoch: 3200, acc: 0.7566666666666667, loss: 0.507382\n",
            "epoch: 3300, acc: 0.7666666666666667, loss: 0.49810567\n",
            "epoch: 3400, acc: 0.7466666666666667, loss: 0.50545835\n",
            "epoch: 3500, acc: 0.7533333333333333, loss: 0.52785236\n",
            "epoch: 3600, acc: 0.7633333333333333, loss: 0.5011049\n",
            "epoch: 3700, acc: 0.5933333333333334, loss: 0.95652384\n",
            "epoch: 3800, acc: 0.7533333333333333, loss: 0.54543\n",
            "epoch: 3900, acc: 0.77, loss: 0.51991314\n",
            "epoch: 4000, acc: 0.7466666666666667, loss: 0.55105364\n",
            "epoch: 4100, acc: 0.77, loss: 0.5251275\n",
            "epoch: 4200, acc: 0.7533333333333333, loss: 0.5509667\n",
            "epoch: 4300, acc: 0.7633333333333333, loss: 0.52993447\n",
            "epoch: 4400, acc: 0.7733333333333333, loss: 0.5211625\n",
            "epoch: 4500, acc: 0.75, loss: 0.5617282\n",
            "epoch: 4600, acc: 0.7666666666666667, loss: 0.5254457\n",
            "epoch: 4700, acc: 0.75, loss: 0.5464226\n",
            "epoch: 4800, acc: 0.7433333333333333, loss: 0.56505084\n",
            "epoch: 4900, acc: 0.7533333333333333, loss: 0.5561443\n",
            "epoch: 5000, acc: 0.7533333333333333, loss: 0.5597045\n",
            "epoch: 5100, acc: 0.7333333333333333, loss: 0.5690752\n",
            "epoch: 5200, acc: 0.7733333333333333, loss: 0.471844\n",
            "epoch: 5300, acc: 0.7766666666666666, loss: 0.4976223\n",
            "epoch: 5400, acc: 0.76, loss: 0.51593006\n",
            "epoch: 5500, acc: 0.7633333333333333, loss: 0.5198641\n",
            "epoch: 5600, acc: 0.6966666666666667, loss: 0.65337527\n",
            "epoch: 5700, acc: 0.7466666666666667, loss: 0.55736834\n",
            "epoch: 5800, acc: 0.7666666666666667, loss: 0.5081805\n",
            "epoch: 5900, acc: 0.7633333333333333, loss: 0.53817695\n",
            "epoch: 6000, acc: 0.7566666666666667, loss: 0.5132074\n",
            "epoch: 6100, acc: 0.7766666666666666, loss: 0.49863556\n",
            "epoch: 6200, acc: 0.7666666666666667, loss: 0.5359544\n",
            "epoch: 6300, acc: 0.7633333333333333, loss: 0.51417255\n",
            "epoch: 6400, acc: 0.75, loss: 0.5750256\n",
            "epoch: 6500, acc: 0.8, loss: 0.45624694\n",
            "epoch: 6600, acc: 0.7566666666666667, loss: 0.55197734\n",
            "epoch: 6700, acc: 0.7633333333333333, loss: 0.4881286\n",
            "epoch: 6800, acc: 0.7766666666666666, loss: 0.4849469\n",
            "epoch: 6900, acc: 0.7533333333333333, loss: 0.5434448\n",
            "epoch: 7000, acc: 0.7566666666666667, loss: 0.51606935\n",
            "epoch: 7100, acc: 0.7733333333333333, loss: 0.530923\n",
            "epoch: 7200, acc: 0.7866666666666666, loss: 0.4998066\n",
            "epoch: 7300, acc: 0.7666666666666667, loss: 0.5208546\n",
            "epoch: 7400, acc: 0.75, loss: 0.5913459\n",
            "epoch: 7500, acc: 0.7666666666666667, loss: 0.5482794\n",
            "epoch: 7600, acc: 0.7533333333333333, loss: 0.5081437\n",
            "epoch: 7700, acc: 0.6866666666666666, loss: 0.6927972\n",
            "epoch: 7800, acc: 0.7666666666666667, loss: 0.53892905\n",
            "epoch: 7900, acc: 0.7633333333333333, loss: 0.5435107\n",
            "epoch: 8000, acc: 0.79, loss: 0.4811328\n",
            "epoch: 8100, acc: 0.77, loss: 0.5119162\n",
            "epoch: 8200, acc: 0.7733333333333333, loss: 0.50882375\n",
            "epoch: 8300, acc: 0.7566666666666667, loss: 0.546741\n",
            "epoch: 8400, acc: 0.7466666666666667, loss: 0.5209506\n",
            "epoch: 8500, acc: 0.7733333333333333, loss: 0.5187918\n",
            "epoch: 8600, acc: 0.5666666666666667, loss: 1.6868304\n",
            "epoch: 8700, acc: 0.7233333333333334, loss: 0.68185407\n",
            "epoch: 8800, acc: 0.7766666666666666, loss: 0.49203023\n",
            "epoch: 8900, acc: 0.7666666666666667, loss: 0.5442606\n",
            "epoch: 9000, acc: 0.7766666666666666, loss: 0.48902613\n",
            "epoch: 9100, acc: 0.7866666666666666, loss: 0.49672043\n",
            "epoch: 9200, acc: 0.7966666666666666, loss: 0.52604717\n",
            "epoch: 9300, acc: 0.7566666666666667, loss: 0.5564551\n",
            "epoch: 9400, acc: 0.6533333333333333, loss: 1.1500785\n",
            "epoch: 9500, acc: 0.79, loss: 0.5015679\n",
            "epoch: 9600, acc: 0.81, loss: 0.4513439\n",
            "epoch: 9700, acc: 0.7233333333333334, loss: 0.61651564\n",
            "epoch: 9800, acc: 0.7833333333333333, loss: 0.48249832\n",
            "epoch: 9900, acc: 0.8033333333333333, loss: 0.48217952\n",
            "epoch: 10000, acc: 0.5166666666666667, loss: 2.7060966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCKRd-zbz4bj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}