{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Networks_from_Scratch_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mgRKt07wg43",
        "outputId": "e68f5b5c-78b1-4874-95b5-6bf31ca28075"
      },
      "source": [
        "# !pip install nnfs"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nnfs\n",
            "  Downloading https://files.pythonhosted.org/packages/06/8c/3003a41d5229e65da792331b060dcad8100a0a5b9760f8c2074cde864148/nnfs-0.5.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nnfs) (1.19.5)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebmUv1xfwwn6"
      },
      "source": [
        "**L1 and L2 Regularization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LajZPb_SwwS9"
      },
      "source": [
        "# Dense layer\n",
        "class Layer_Dense :\n",
        "  # Layer initialization\n",
        "  def __init__ ( self , n_inputs , n_neurons , \n",
        "                weight_regularizer_l1 = 0 , weight_regularizer_l2 = 0 , \n",
        "                bias_regularizer_l1 = 0 , bias_regularizer_l2 = 0 \n",
        "                ):\n",
        "    # Initialize weights and biases\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros(( 1 , n_neurons))\n",
        "\n",
        "    # Set regularization strength\n",
        "    self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "    self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "    self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "    self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "\n",
        "  def forward ( self , inputs ):\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues ):\n",
        "    # Gradients on parameters\n",
        "    self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "    self.dbiases = np.sum(dvalues, axis = 0 , keepdims = True )\n",
        "\n",
        "    # Gradients on regularization\n",
        "    # L1 on weights\n",
        "    if self.weight_regularizer_l1 > 0 :\n",
        "      dL1 = np.ones_like(self.weights)\n",
        "      dL1[self.weights < 0 ] = - 1\n",
        "      self.dweights += self.weight_regularizer_l1 * dL1\n",
        "\n",
        "    # L2 on weights\n",
        "    if self.weight_regularizer_l2 > 0 :\n",
        "      self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
        "\n",
        "    # L1 on biases\n",
        "    if self.bias_regularizer_l1 > 0 :\n",
        "      dL1 = np.ones_like(self.biases)\n",
        "      dL1[self.biases < 0 ] = - 1\n",
        "      self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "\n",
        "    # L2 on biases\n",
        "    if self.bias_regularizer_l2 > 0 :\n",
        "      self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
        "\n",
        "    # Gradient on values\n",
        "    self.dinputs = np.dot(dvalues, self.weights.T)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKsXYLddwcUv"
      },
      "source": [
        "# Common loss class\n",
        "class Loss :  \n",
        "  # Calculates the data and regularization losses\n",
        "  # given model output and ground truth values\n",
        "  def calculate ( self , output , y ):\n",
        "    # Calculate sample losses\n",
        "    sample_losses = self.forward(output, y)\n",
        "\n",
        "    # Calculate mean loss\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    \n",
        "    # Return loss\n",
        "    return data_loss\n",
        "\n",
        "  # Regularization loss calculation\n",
        "  def regularization_loss ( self , layer ):\n",
        "    # 0 by default\n",
        "    regularization_loss = 0\n",
        "\n",
        "    # L1 regularization - weights\n",
        "    # calculate only when factor greater than 0\n",
        "    if layer.weight_regularizer_l1 > 0 :\n",
        "      regularization_loss += layer.weight_regularizer_l1 *  np.sum(np.abs(layer.weights))\n",
        "\n",
        "    # L2 regularization - weights\n",
        "    if layer.weight_regularizer_l2 > 0 :\n",
        "      regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
        "\n",
        "    # L1 regularization - biases\n",
        "    # calculate only when factor greater than 0\n",
        "    if layer.bias_regularizer_l1 > 0 :\n",
        "      regularization_loss += layer.bias_regularizer_l1 *  np.sum(np.abs(layer.biases))\n",
        "      \n",
        "    # L2 regularization - biases\n",
        "    if layer.bias_regularizer_l2 > 0 :\n",
        "      regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
        "    \n",
        "    return regularization_loss"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIJhIOR88qhf"
      },
      "source": [
        "***Complete Code with Regularization:***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkbO_Key52XB"
      },
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "nnfs.init()\n",
        "\n",
        "# Dense layer\n",
        "class Layer_Dense :\n",
        "  # Layer initialization\n",
        "  def __init__ ( self , n_inputs , n_neurons ,\n",
        "    weight_regularizer_l1 = 0 , weight_regularizer_l2 = 0 ,\n",
        "    bias_regularizer_l1 = 0 , bias_regularizer_l2 = 0 ):\n",
        "    # Initialize weights and biases\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros(( 1 , n_neurons))\n",
        "\n",
        "    # Set regularization strength\n",
        "    self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "    self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "    self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "    self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "    \n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Remember input values\n",
        "    self.inputs = inputs\n",
        "    \n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues ):\n",
        "    # Gradients on parameters\n",
        "    self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "    self.dbiases = np.sum(dvalues, axis = 0 , keepdims = True )\n",
        "\n",
        "    # Gradients on regularization\n",
        "    # L1 on weights\n",
        "    if self.weight_regularizer_l1 > 0 :\n",
        "      dL1 = np.ones_like(self.weights)\n",
        "      dL1[ self.weights < 0 ] = - 1\n",
        "      self.dweights += self.weight_regularizer_l1 * dL1\n",
        "\n",
        "    # L2 on weights\n",
        "    if self.weight_regularizer_l2 > 0 :\n",
        "      self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
        "\n",
        "    # L1 on biases\n",
        "    if self.bias_regularizer_l1 > 0 :\n",
        "      dL1 = np.ones_like(self.biases)\n",
        "      dL1[ self.biases < 0 ] = - 1\n",
        "      self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "\n",
        "    # L2 on biases\n",
        "    if self.bias_regularizer_l2 > 0 :\n",
        "      self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
        "\n",
        "    # Gradient on values\n",
        "    self.dinputs = np.dot(dvalues, self.weights.T) \n",
        "\n",
        "# ReLU activation\n",
        "class Activation_ReLU :\n",
        "  # Forward pass\n",
        "  def forward( self , inputs ):\n",
        "    # Remember input values\n",
        "    self.inputs = inputs\n",
        "    # Calculate output values from inputs\n",
        "    self.output = np.maximum( 0 , inputs)\n",
        "\n",
        "  # Backward pass\n",
        "  def backward( self , dvalues ):\n",
        "    # Since we need to modify the original variable,\n",
        "    # let's make a copy of the values first\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Zero gradient where input values were negative\n",
        "    self.dinputs[self.inputs <= 0 ] = 0\n",
        "\n",
        "# Softmax activation\n",
        "class Activation_Softmax :\n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Get unnormalized probabilities\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis = 1 , keepdims = True ))\n",
        "\n",
        "    # Normalize them for each sample\n",
        "    probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True )\n",
        "    \n",
        "    self.output = probabilities\n",
        "\n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues ):\n",
        "    # Create uninitialized array\n",
        "    self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "    # Enumerate outputs and gradients\n",
        "    for index, (single_output, single_dvalues) in enumerate ( zip (self.output, dvalues)):\n",
        "      # Flatten output array\n",
        "      single_output = single_output.reshape( - 1 , 1 )\n",
        "\n",
        "      # Calculate Jacobian matrix of the output and\n",
        "      jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "\n",
        "      # Calculate sample-wise gradient\n",
        "      # and add it to the array of sample gradients\n",
        "      self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
        "\n",
        "# SGD optimizer\n",
        "class Optimizer_SGD :\n",
        "  # Initialize optimizer - set settings,\n",
        "  # learning rate of 1. is default for this optimizer\n",
        "  def __init__ ( self , learning_rate = 1. , decay = 0. , momentum = 0. ):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.momentum = momentum\n",
        "\n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params ( self ):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * ( 1. / ( 1. + self.decay * self.iterations))\n",
        "\n",
        "  # Update parameters\n",
        "  def update_params ( self , layer ):\n",
        "    # If we use momentum\n",
        "    if self.momentum:\n",
        "      # If layer does not contain momentum arrays, create them\n",
        "      # filled with zeros\n",
        "      if not hasattr (layer, 'weight_momentums'):\n",
        "        layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "        # If there is no momentum array for weights\n",
        "        # The array doesn't exist for biases yet either.\n",
        "        layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "      # Build weight updates with momentum - take previous\n",
        "      # updates multiplied by retain factor and update with\n",
        "      # current gradients\n",
        "      weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
        "      layer.weight_momentums = weight_updates\n",
        "\n",
        "      # Build bias updates\n",
        "      bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
        "      layer.bias_momentums = bias_updates\n",
        "    # Vanilla SGD updates (as before momentum update)\n",
        "    else :\n",
        "      weight_updates = - self.current_learning_rate * layer.dweights\n",
        "      bias_updates = - self.current_learning_rate * layer.dbiases\n",
        "    \n",
        "    # Update weights and biases using either\n",
        "    # vanilla or momentum updates\n",
        "    layer.weights += weight_updates\n",
        "    layer.biases += bias_updates\n",
        "      \n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params ( self ):\n",
        "    self.iterations += 1\n",
        "\n",
        "# Adagrad optimizer\n",
        "class Optimizer_Adagrad :\n",
        "  # Initialize optimizer - set settings\n",
        "  def __init__ ( self , learning_rate = 1. , decay = 0. , epsilon = 1e-7 ):\n",
        "      self.learning_rate = learning_rate\n",
        "      self.current_learning_rate = learning_rate\n",
        "      self.decay = decay\n",
        "      self.iterations = 0\n",
        "      self.epsilon = epsilon\n",
        "\n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params ( self ):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * ( 1. / ( 1. + self.decay * self.iterations))\n",
        "  \n",
        "  # Update parameters\n",
        "  def update_params ( self , layer ):\n",
        "    # If layer does not contain cache arrays,\n",
        "    # create them filled with zeros\n",
        "    if not hasattr (layer, 'weight_cache' ):\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "    # Update cache with squared current gradients\n",
        "    layer.weight_cache += layer.dweights ** 2\n",
        "    layer.bias_cache += layer.dbiases ** 2\n",
        "\n",
        "    # Vanilla SGD parameter update + normalization\n",
        "    # with square rooted cache\n",
        "    layer.weights += - self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "    layer.biases += - self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params ( self ):\n",
        "    self.iterations += 1\n",
        "\n",
        "# RMSprop optimizer\n",
        "class Optimizer_RMSprop :\n",
        "  # Initialize optimizer - set settings\n",
        "  def __init__ ( self , learning_rate = 0.001 , decay = 0. , epsilon = 1e-7 , rho = 0.9 ):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "    self.rho = rho\n",
        "\n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params ( self ):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * ( 1. / ( 1. + self.decay * self.iterations))\n",
        "\n",
        "  # Update parameters\n",
        "  def update_params ( self , layer ):\n",
        "    # If layer does not contain cache arrays,\n",
        "    # create them filled with zeros\n",
        "    if not hasattr (layer, 'weight_cache' ):\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "    # Update cache with squared current gradients\n",
        "    layer.weight_cache = self.rho * layer.weight_cache + ( 1 - self.rho) * layer.dweights ** 2\n",
        "    layer.bias_cache = self.rho * layer.bias_cache + ( 1 - self.rho) * layer.dbiases ** 2\n",
        "\n",
        "    # Vanilla SGD parameter update + normalization\n",
        "    # with square rooted cache\n",
        "    layer.weights += - self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "    layer.biases += - self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params ( self ):\n",
        "    self.iterations += 1\n",
        "\n",
        "\n",
        "# Adam optimizer\n",
        "class Optimizer_Adam :\n",
        "  # Initialize optimizer - set settings\n",
        "  def __init__ ( self , learning_rate = 0.001 , decay = 0. , epsilon = 1e-7 ,\n",
        "    beta_1 = 0.9 , beta_2 = 0.999 ):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "\n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params ( self ):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * ( 1. / ( 1. + self.decay * self.iterations))\n",
        "\n",
        "  # Update parameters\n",
        "  def update_params ( self , layer ):\n",
        "    # If layer does not contain cache arrays,\n",
        "    # create them filled with zeros\n",
        "    if not hasattr (layer, 'weight_cache' ):\n",
        "      layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "    # Update momentum with current gradients\n",
        "    layer.weight_momentums = self.beta_1 * layer.weight_momentums + ( 1 - self.beta_1) * layer.dweights\n",
        "    layer.bias_momentums = self.beta_1 * layer.bias_momentums + ( 1 - self.beta_1) * layer.dbiases\n",
        "\n",
        "    # Get corrected momentum\n",
        "    # self.iteration is 0 at first pass\n",
        "    # and we need to start with 1 here\n",
        "    weight_momentums_corrected = layer.weight_momentums / ( 1 - self.beta_1 ** (self.iterations + 1 ))\n",
        "    bias_momentums_corrected = layer.bias_momentums / ( 1 - self.beta_1 ** (self.iterations + 1 ))\n",
        "\n",
        "    # Update cache with squared current gradients\n",
        "    layer.weight_cache = self.beta_2 * layer.weight_cache + ( 1 - self.beta_2) * layer.dweights ** 2\n",
        "    layer.bias_cache = self.beta_2 * layer.bias_cache + ( 1 - self.beta_2) * layer.dbiases ** 2\n",
        "\n",
        "    # Get corrected cache\n",
        "    weight_cache_corrected = layer.weight_cache / ( 1 - self.beta_2 ** (self.iterations + 1 ))\n",
        "    bias_cache_corrected = layer.bias_cache / ( 1 - self.beta_2 ** (self.iterations + 1 ))\n",
        "\n",
        "    # Vanilla SGD parameter update + normalization\n",
        "    # with square rooted cache\n",
        "    layer.weights += - self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
        "    layer.biases += - self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
        "\n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params ( self ):\n",
        "    self.iterations += 1\n",
        "\n",
        "# Common loss class\n",
        "class Loss :\n",
        "  # Regularization loss calculation\n",
        "  def regularization_loss ( self , layer ):\n",
        "    # 0 by default\n",
        "    regularization_loss = 0\n",
        "\n",
        "    # L1 regularization - weights\n",
        "    # calculate only when factor greater than 0\n",
        "    if layer.weight_regularizer_l1 > 0 :\n",
        "      regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
        "\n",
        "    # L2 regularization - weights\n",
        "    if layer.weight_regularizer_l2 > 0 :\n",
        "      regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
        "\n",
        "    # L1 regularization - biases\n",
        "    # calculate only when factor greater than 0\n",
        "    if layer.bias_regularizer_l1 > 0 :\n",
        "      regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
        "\n",
        "    # L2 regularization - biases\n",
        "    if layer.bias_regularizer_l2 > 0 :\n",
        "      regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
        "\n",
        "    return regularization_loss\n",
        "\n",
        "  # Calculates the data and regularization losses\n",
        "  # given model output and ground truth values\n",
        "  def calculate ( self , output , y ):\n",
        "    # Calculate sample losses\n",
        "    sample_losses = self.forward(output, y)\n",
        "\n",
        "    # Calculate mean loss\n",
        "    data_loss = np.mean(sample_losses)\n",
        "\n",
        "    # Return loss\n",
        "    return data_loss\n",
        "\n",
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy ( Loss ):\n",
        "  # Forward pass\n",
        "  def forward( self , y_pred , y_true ):\n",
        "    # Number of samples in a batch\n",
        "    samples = len(y_pred)\n",
        "\n",
        "    # Clip data to prevent division by 0\n",
        "    # Clip both sides to not drag mean towards any value\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7 , 1 - 1e-7 )\n",
        "\n",
        "    # Probabilities for target values -\n",
        "    # only if categorical labels\n",
        "    if len(y_true.shape) == 1 :\n",
        "      correct_confidences = y_pred_clipped[range(samples),y_true]\n",
        "    elif len(y_true.shape) == 2 : # Mask values - only for one-hot encoded labels\n",
        "      correct_confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
        "\n",
        "    # Losses\n",
        "    negative_log_likelihoods = - np.log(correct_confidences)\n",
        "\n",
        "    return negative_log_likelihoods\n",
        "\n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues , y_true ):\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "\n",
        "    # Number of labels in every sample\n",
        "    # We'll use the first sample to count them\n",
        "    labels = len(dvalues[ 0 ])\n",
        "\n",
        "    # If labels are sparse, turn them into one-hot vector\n",
        "    if len(y_true.shape) == 1 :\n",
        "      y_true = np.eye(labels)[y_true]\n",
        "\n",
        "    # Calculate gradient\n",
        "    self.dinputs = - y_true / dvalues\n",
        "\n",
        "    # Normalize gradient\n",
        "    self.dinputs = self.dinputs / samples\n",
        "\n",
        "# Softmax classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy ():\n",
        "  # Creates activation and loss function objects\n",
        "  def __init__( self ):\n",
        "    self.activation = Activation_Softmax()\n",
        "    self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "  # Forward pass\n",
        "  def forward( self , inputs , y_true ):\n",
        "    # Output layer's activation function\n",
        "    self.activation.forward(inputs)\n",
        "    # Set the output\n",
        "    self.output = self.activation.output\n",
        "    # Calculate and return loss value\n",
        "    return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "  # Backward pass\n",
        "  def backward( self , dvalues , y_true ):\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "\n",
        "    # If labels are one-hot encoded,\n",
        "    # turn them into discrete values\n",
        "    if len(y_true.shape) == 2 :\n",
        "      y_true = np.argmax(y_true, axis = 1)\n",
        "\n",
        "    # Copy so we can safely modify\n",
        "    self.dinputs = dvalues.copy()\n",
        "\n",
        "    # Calculate gradient\n",
        "    self.dinputs[ range (samples), y_true] -= 1\n",
        "\n",
        "    # Normalize gradient\n",
        "    self.dinputs = self.dinputs / samples\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVxkhhx-8zJQ",
        "outputId": "8751f532-e132-45b3-f55b-0469b8b5f2d4"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data( samples = 100 , classes = 3 )\n",
        "\n",
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense( 2 , 64 , weight_regularizer_l2 = 5e-4 , bias_regularizer_l2 = 5e-4 )\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 64 input features (as we take output\n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense( 64 , 3 )\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_Adam( learning_rate = 0.02 , decay = 5e-7 )\n",
        "\n",
        "# Train in loop\n",
        "for epoch in range ( 10001 ):\n",
        "  # Perform a forward pass of our training data through this layer\n",
        "  dense1.forward(X)\n",
        "\n",
        "  # Perform a forward pass through activation function\n",
        "  # takes the output of first dense layer here\n",
        "  activation1.forward(dense1.output)\n",
        "\n",
        "  # Perform a forward pass through second Dense layer\n",
        "  # takes outputs of activation function of first layer as inputs\n",
        "  dense2.forward(activation1.output)\n",
        "\n",
        "  # Perform a forward pass through the activation/loss function\n",
        "  # takes the output of second dense layer here and returns loss\n",
        "  data_loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "  # Calculate regularization penalty\n",
        "  regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2)\n",
        "\n",
        "  # Calculate overall loss\n",
        "  loss = data_loss + regularization_loss\n",
        "\n",
        "  # Calculate accuracy from output of activation2 and targets\n",
        "  # calculate values along first axis\n",
        "  predictions = np.argmax(loss_activation.output, axis = 1 )\n",
        "\n",
        "  if len (y.shape) == 2 :\n",
        "    y = np.argmax(y, axis = 1 )\n",
        "\n",
        "  accuracy = np.mean(predictions == y)\n",
        "  if not epoch % 100 :\n",
        "    print ( 'epoch: '+ str(epoch) + ' , ' +\n",
        "           'acc: ' + str(accuracy) + ' , ' +\n",
        "           'loss: ' + str(loss)  + ' , ' +\n",
        "           'data_loss: ' + str(data_loss)  + ' , ' +\n",
        "           'reg_loss: ' + str(regularization_loss)  + ' , ' +\n",
        "           'lr: ' + str(optimizer.current_learning_rate) )\n",
        "    \n",
        "  # Backward pass\n",
        "  loss_activation.backward(loss_activation.output, y)\n",
        "  dense2.backward(loss_activation.dinputs)\n",
        "  activation1.backward(dense2.dinputs)\n",
        "  dense1.backward(activation1.dinputs)\n",
        "\n",
        "  # Update weights and biases\n",
        "  optimizer.pre_update_params()\n",
        "  optimizer.update_params(dense1)\n",
        "  optimizer.update_params(dense2)\n",
        "  optimizer.post_update_params()\n",
        "  "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 , acc: 0.36 , loss: 1.0986004566168412 , data_loss: 1.0985943 , reg_loss: 6.1487173661589625e-06 , lr: 0.02\n",
            "epoch: 100 , acc: 0.69 , loss: 0.8449501132965088 , data_loss: 0.8068745 , reg_loss: 0.03807559967041016 , lr: 0.019999010049002574\n",
            "epoch: 200 , acc: 0.7733333333333333 , loss: 0.6895837779045105 , data_loss: 0.61419386 , reg_loss: 0.0753899211883545 , lr: 0.019998010197985302\n",
            "epoch: 300 , acc: 0.8166666666666667 , loss: 0.6171496715545655 , data_loss: 0.5256841 , reg_loss: 0.09146555328369141 , lr: 0.019997010446938183\n",
            "epoch: 400 , acc: 0.8233333333333334 , loss: 0.5673076455593109 , data_loss: 0.46879253 , reg_loss: 0.09851511764526368 , lr: 0.01999601079584623\n",
            "epoch: 500 , acc: 0.83 , loss: 0.5345990812778473 , data_loss: 0.43236867 , reg_loss: 0.10223041534423828 , lr: 0.01999501124469445\n",
            "epoch: 600 , acc: 0.8433333333333334 , loss: 0.50355228805542 , data_loss: 0.39933777 , reg_loss: 0.10421451950073242 , lr: 0.01999401179346786\n",
            "epoch: 700 , acc: 0.8566666666666667 , loss: 0.48096590924263 , data_loss: 0.37610427 , reg_loss: 0.10486164379119874 , lr: 0.01999301244215147\n",
            "epoch: 800 , acc: 0.8566666666666667 , loss: 0.4617595686912537 , data_loss: 0.35725814 , reg_loss: 0.10450142765045167 , lr: 0.0199920131907303\n",
            "epoch: 900 , acc: 0.8733333333333333 , loss: 0.44801131939888 , data_loss: 0.3444983 , reg_loss: 0.10351301288604738 , lr: 0.019991014039189386\n",
            "epoch: 1000 , acc: 0.87 , loss: 0.43366239857673644 , data_loss: 0.3313792 , reg_loss: 0.10228319358825684 , lr: 0.019990014987513734\n",
            "epoch: 1100 , acc: 0.88 , loss: 0.42237355184555053 , data_loss: 0.32124704 , reg_loss: 0.1011265106201172 , lr: 0.01998901603568839\n",
            "epoch: 1200 , acc: 0.8833333333333333 , loss: 0.41250973773002625 , data_loss: 0.312623 , reg_loss: 0.09988674354553223 , lr: 0.019988017183698373\n",
            "epoch: 1300 , acc: 0.8866666666666667 , loss: 0.40342678570747376 , data_loss: 0.30479267 , reg_loss: 0.09863411331176758 , lr: 0.01998701843152872\n",
            "epoch: 1400 , acc: 0.89 , loss: 0.3958400321006775 , data_loss: 0.29850072 , reg_loss: 0.09733931541442871 , lr: 0.019986019779164473\n",
            "epoch: 1500 , acc: 0.8933333333333333 , loss: 0.3887316761016846 , data_loss: 0.29253304 , reg_loss: 0.09619863605499268 , lr: 0.019985021226590672\n",
            "epoch: 1600 , acc: 0.9 , loss: 0.38123025846481323 , data_loss: 0.28637117 , reg_loss: 0.09485908699035644 , lr: 0.01998402277379235\n",
            "epoch: 1700 , acc: 0.8966666666666666 , loss: 0.37687057685852055 , data_loss: 0.28327847 , reg_loss: 0.09359211158752442 , lr: 0.01998302442075457\n",
            "epoch: 1800 , acc: 0.9 , loss: 0.36962746262550356 , data_loss: 0.27721646 , reg_loss: 0.09241099834442139 , lr: 0.019982026167462367\n",
            "epoch: 1900 , acc: 0.9 , loss: 0.36404252648353574 , data_loss: 0.27280024 , reg_loss: 0.09124228954315185 , lr: 0.019981028013900805\n",
            "epoch: 2000 , acc: 0.9033333333333333 , loss: 0.36006971764564516 , data_loss: 0.27002504 , reg_loss: 0.09004467296600342 , lr: 0.019980029960054924\n",
            "epoch: 2100 , acc: 0.9066666666666666 , loss: 0.355933783531189 , data_loss: 0.26702154 , reg_loss: 0.08891224670410157 , lr: 0.019979032005909798\n",
            "epoch: 2200 , acc: 0.9 , loss: 0.3525214459896088 , data_loss: 0.26480827 , reg_loss: 0.08771317863464356 , lr: 0.01997803415145048\n",
            "epoch: 2300 , acc: 0.9033333333333333 , loss: 0.3487828280925751 , data_loss: 0.26217505 , reg_loss: 0.08660777473449707 , lr: 0.019977036396662037\n",
            "epoch: 2400 , acc: 0.9033333333333333 , loss: 0.3435909652709961 , data_loss: 0.25806916 , reg_loss: 0.08552180767059327 , lr: 0.019976038741529537\n",
            "epoch: 2500 , acc: 0.92 , loss: 0.3378697421550751 , data_loss: 0.2533197 , reg_loss: 0.08455003166198731 , lr: 0.01997504118603805\n",
            "epoch: 2600 , acc: 0.8933333333333333 , loss: 0.34290600490570067 , data_loss: 0.25921178 , reg_loss: 0.08369422626495361 , lr: 0.01997404373017264\n",
            "epoch: 2700 , acc: 0.9 , loss: 0.3323246808052063 , data_loss: 0.24951392 , reg_loss: 0.08281075668334961 , lr: 0.0199730463739184\n",
            "epoch: 2800 , acc: 0.92 , loss: 0.32661374855041503 , data_loss: 0.24463654 , reg_loss: 0.08197721290588379 , lr: 0.019972049117260395\n",
            "epoch: 2900 , acc: 0.9066666666666666 , loss: 0.32404780995845794 , data_loss: 0.2429124 , reg_loss: 0.08113541316986085 , lr: 0.019971051960183714\n",
            "epoch: 3000 , acc: 0.9066666666666666 , loss: 0.32137118935585024 , data_loss: 0.24103478 , reg_loss: 0.08033641338348389 , lr: 0.019970054902673444\n",
            "epoch: 3100 , acc: 0.9066666666666666 , loss: 0.31845608496665956 , data_loss: 0.238924 , reg_loss: 0.07953208827972412 , lr: 0.019969057944714663\n",
            "epoch: 3200 , acc: 0.91 , loss: 0.3141741371154785 , data_loss: 0.23536563 , reg_loss: 0.07880850791931152 , lr: 0.019968061086292475\n",
            "epoch: 3300 , acc: 0.9066666666666666 , loss: 0.3106368343830109 , data_loss: 0.23256305 , reg_loss: 0.07807378578186036 , lr: 0.019967064327391967\n",
            "epoch: 3400 , acc: 0.9166666666666666 , loss: 0.31072455799579624 , data_loss: 0.23340459 , reg_loss: 0.07731996631622315 , lr: 0.019966067667998237\n",
            "epoch: 3500 , acc: 0.9166666666666666 , loss: 0.30591959488391873 , data_loss: 0.2292424 , reg_loss: 0.07667719554901123 , lr: 0.019965071108096383\n",
            "epoch: 3600 , acc: 0.9233333333333333 , loss: 0.3036388342380524 , data_loss: 0.2273697 , reg_loss: 0.07626913833618164 , lr: 0.01996407464767152\n",
            "epoch: 3700 , acc: 0.92 , loss: 0.30077094745635985 , data_loss: 0.22513318 , reg_loss: 0.07563776683807373 , lr: 0.019963078286708732\n",
            "epoch: 3800 , acc: 0.92 , loss: 0.29675088787078857 , data_loss: 0.22174537 , reg_loss: 0.0750055160522461 , lr: 0.019962082025193145\n",
            "epoch: 3900 , acc: 0.9066666666666666 , loss: 0.29651286005973815 , data_loss: 0.2221491 , reg_loss: 0.07436375617980957 , lr: 0.019961085863109868\n",
            "epoch: 4000 , acc: 0.9133333333333333 , loss: 0.29229452180862425 , data_loss: 0.21858007 , reg_loss: 0.0737144546508789 , lr: 0.019960089800444013\n",
            "epoch: 4100 , acc: 0.9133333333333333 , loss: 0.2915211002826691 , data_loss: 0.2184147 , reg_loss: 0.07310640621185303 , lr: 0.019959093837180697\n",
            "epoch: 4200 , acc: 0.9166666666666666 , loss: 0.28954761719703676 , data_loss: 0.21697703 , reg_loss: 0.07257058715820312 , lr: 0.01995809797330505\n",
            "epoch: 4300 , acc: 0.93 , loss: 0.2941644229888916 , data_loss: 0.22221255 , reg_loss: 0.07195186996459961 , lr: 0.01995710220880218\n",
            "epoch: 4400 , acc: 0.9233333333333333 , loss: 0.28429252326488497 , data_loss: 0.21291451 , reg_loss: 0.07137801170349121 , lr: 0.019956106543657228\n",
            "epoch: 4500 , acc: 0.91 , loss: 0.2910678625106812 , data_loss: 0.22025907 , reg_loss: 0.07080879211425781 , lr: 0.019955110977855316\n",
            "epoch: 4600 , acc: 0.9266666666666666 , loss: 0.2802449852228165 , data_loss: 0.20996775 , reg_loss: 0.07027723789215089 , lr: 0.01995411551138158\n",
            "epoch: 4700 , acc: 0.8533333333333334 , loss: 0.5027539122104645 , data_loss: 0.42894283 , reg_loss: 0.07381108283996582 , lr: 0.019953120144221154\n",
            "epoch: 4800 , acc: 0.9266666666666666 , loss: 0.28248662447929385 , data_loss: 0.20880195 , reg_loss: 0.07368466949462892 , lr: 0.019952124876359174\n",
            "epoch: 4900 , acc: 0.93 , loss: 0.28020310735702514 , data_loss: 0.20706934 , reg_loss: 0.07313376998901368 , lr: 0.01995112970778079\n",
            "epoch: 5000 , acc: 0.93 , loss: 0.27875969183444976 , data_loss: 0.20607199 , reg_loss: 0.07268770408630372 , lr: 0.019950134638471142\n",
            "epoch: 5100 , acc: 0.93 , loss: 0.27748221743106843 , data_loss: 0.20514895 , reg_loss: 0.07233326721191406 , lr: 0.019949139668415376\n",
            "epoch: 5200 , acc: 0.93 , loss: 0.2762230563163757 , data_loss: 0.20421118 , reg_loss: 0.0720118808746338 , lr: 0.01994814479759864\n",
            "epoch: 5300 , acc: 0.93 , loss: 0.27518690836429593 , data_loss: 0.20353343 , reg_loss: 0.07165348243713379 , lr: 0.019947150026006097\n",
            "epoch: 5400 , acc: 0.93 , loss: 0.2739205332994461 , data_loss: 0.20258565 , reg_loss: 0.07133488082885742 , lr: 0.019946155353622895\n",
            "epoch: 5500 , acc: 0.93 , loss: 0.27217855155467985 , data_loss: 0.20113032 , reg_loss: 0.07104823589324952 , lr: 0.019945160780434196\n",
            "epoch: 5600 , acc: 0.93 , loss: 0.2702116092443466 , data_loss: 0.19940875 , reg_loss: 0.07080285453796387 , lr: 0.019944166306425162\n",
            "epoch: 5700 , acc: 0.93 , loss: 0.2688824541568756 , data_loss: 0.1982961 , reg_loss: 0.07058635425567628 , lr: 0.01994317193158096\n",
            "epoch: 5800 , acc: 0.93 , loss: 0.2676947691440582 , data_loss: 0.19734803 , reg_loss: 0.07034674072265625 , lr: 0.019942177655886757\n",
            "epoch: 5900 , acc: 0.93 , loss: 0.2664252172708511 , data_loss: 0.19633164 , reg_loss: 0.07009358215332032 , lr: 0.019941183479327725\n",
            "epoch: 6000 , acc: 0.93 , loss: 0.26524510204792023 , data_loss: 0.19543655 , reg_loss: 0.06980854988098145 , lr: 0.019940189401889033\n",
            "epoch: 6100 , acc: 0.93 , loss: 0.2639214669466019 , data_loss: 0.19435973 , reg_loss: 0.0695617322921753 , lr: 0.01993919542355587\n",
            "epoch: 6200 , acc: 0.93 , loss: 0.2626279774904251 , data_loss: 0.19335051 , reg_loss: 0.06927746868133546 , lr: 0.019938201544313403\n",
            "epoch: 6300 , acc: 0.93 , loss: 0.26127032923698423 , data_loss: 0.19227502 , reg_loss: 0.06899531173706054 , lr: 0.01993720776414682\n",
            "epoch: 6400 , acc: 0.9266666666666666 , loss: 0.2599243479967117 , data_loss: 0.1912175 , reg_loss: 0.0687068510055542 , lr: 0.019936214083041307\n",
            "epoch: 6500 , acc: 0.93 , loss: 0.2587662618160248 , data_loss: 0.1903573 , reg_loss: 0.06840896415710448 , lr: 0.01993522050098206\n",
            "epoch: 6600 , acc: 0.93 , loss: 0.25757380735874175 , data_loss: 0.18943395 , reg_loss: 0.06813986015319824 , lr: 0.019934227017954262\n",
            "epoch: 6700 , acc: 0.93 , loss: 0.2562012758255005 , data_loss: 0.18831861 , reg_loss: 0.06788266563415528 , lr: 0.01993323363394311\n",
            "epoch: 6800 , acc: 0.9333333333333333 , loss: 0.25545958256721496 , data_loss: 0.18783215 , reg_loss: 0.06762743568420411 , lr: 0.0199322403489338\n",
            "epoch: 6900 , acc: 0.94 , loss: 0.2541627765893936 , data_loss: 0.18679596 , reg_loss: 0.06736681175231933 , lr: 0.019931247162911534\n",
            "epoch: 7000 , acc: 0.93 , loss: 0.2585901038646698 , data_loss: 0.19152662 , reg_loss: 0.0670634822845459 , lr: 0.019930254075861523\n",
            "epoch: 7100 , acc: 0.9366666666666666 , loss: 0.25127041327953337 , data_loss: 0.18448465 , reg_loss: 0.06678576755523682 , lr: 0.019929261087768962\n",
            "epoch: 7200 , acc: 0.9366666666666666 , loss: 0.24992773175239563 , data_loss: 0.18338588 , reg_loss: 0.0665418529510498 , lr: 0.01992826819861907\n",
            "epoch: 7300 , acc: 0.93 , loss: 0.24918613612651824 , data_loss: 0.18295817 , reg_loss: 0.06622796535491944 , lr: 0.019927275408397054\n",
            "epoch: 7400 , acc: 0.9333333333333333 , loss: 0.2478540736436844 , data_loss: 0.18190832 , reg_loss: 0.06594574928283692 , lr: 0.019926282717088132\n",
            "epoch: 7500 , acc: 0.9366666666666666 , loss: 0.24621527767181398 , data_loss: 0.18048191 , reg_loss: 0.06573336696624757 , lr: 0.01992529012467752\n",
            "epoch: 7600 , acc: 0.9433333333333334 , loss: 0.24598087906837462 , data_loss: 0.18049315 , reg_loss: 0.06548773288726807 , lr: 0.019924297631150445\n",
            "epoch: 7700 , acc: 0.9366666666666666 , loss: 0.2447081400156021 , data_loss: 0.1793449 , reg_loss: 0.06536323261260986 , lr: 0.019923305236492123\n",
            "epoch: 7800 , acc: 0.9466666666666667 , loss: 0.24292144179344177 , data_loss: 0.17788467 , reg_loss: 0.06503677368164062 , lr: 0.01992231294068779\n",
            "epoch: 7900 , acc: 0.9433333333333334 , loss: 0.24267147481441498 , data_loss: 0.17791103 , reg_loss: 0.06476044654846191 , lr: 0.019921320743722666\n",
            "epoch: 8000 , acc: 0.94 , loss: 0.24006995475292206 , data_loss: 0.17556266 , reg_loss: 0.06450728988647461 , lr: 0.019920328645582\n",
            "epoch: 8100 , acc: 0.9466666666666667 , loss: 0.2420288679599762 , data_loss: 0.17781952 , reg_loss: 0.06420934772491455 , lr: 0.019919336646251007\n",
            "epoch: 8200 , acc: 0.94 , loss: 0.23917784345149995 , data_loss: 0.17522787 , reg_loss: 0.06394997787475586 , lr: 0.019918344745714942\n",
            "epoch: 8300 , acc: 0.9366666666666666 , loss: 0.23752018868923186 , data_loss: 0.17338382 , reg_loss: 0.06413637161254883 , lr: 0.019917352943959042\n",
            "epoch: 8400 , acc: 0.9466666666666667 , loss: 0.23489430046081544 , data_loss: 0.17126608 , reg_loss: 0.06362822151184082 , lr: 0.019916361240968555\n",
            "epoch: 8500 , acc: 0.9433333333333334 , loss: 0.23382444775104522 , data_loss: 0.17065938 , reg_loss: 0.06316506958007813 , lr: 0.01991536963672872\n",
            "epoch: 8600 , acc: 0.9466666666666667 , loss: 0.23275153613090516 , data_loss: 0.16990706 , reg_loss: 0.06284447288513184 , lr: 0.019914378131224802\n",
            "epoch: 8700 , acc: 0.95 , loss: 0.23375860273838045 , data_loss: 0.1712705 , reg_loss: 0.062488098144531254 , lr: 0.01991338672444204\n",
            "epoch: 8800 , acc: 0.9433333333333334 , loss: 0.2336061542034149 , data_loss: 0.17138347 , reg_loss: 0.06222268390655518 , lr: 0.0199123954163657\n",
            "epoch: 8900 , acc: 0.9533333333333334 , loss: 0.24260282588005067 , data_loss: 0.17559645 , reg_loss: 0.06700638008117676 , lr: 0.019911404206981037\n",
            "epoch: 9000 , acc: 0.95 , loss: 0.23272112238407136 , data_loss: 0.16649844 , reg_loss: 0.0662226848602295 , lr: 0.019910413096273318\n",
            "epoch: 9100 , acc: 0.95 , loss: 0.23173724269866944 , data_loss: 0.16615117 , reg_loss: 0.0655860767364502 , lr: 0.019909422084227805\n",
            "epoch: 9200 , acc: 0.95 , loss: 0.2310354641675949 , data_loss: 0.16601686 , reg_loss: 0.06501860237121582 , lr: 0.019908431170829768\n",
            "epoch: 9300 , acc: 0.95 , loss: 0.2303180900812149 , data_loss: 0.16584946 , reg_loss: 0.06446862792968751 , lr: 0.01990744035606448\n",
            "epoch: 9400 , acc: 0.95 , loss: 0.2296511951684952 , data_loss: 0.16570045 , reg_loss: 0.06395074462890625 , lr: 0.01990644963991721\n",
            "epoch: 9500 , acc: 0.95 , loss: 0.22908023488521576 , data_loss: 0.16559719 , reg_loss: 0.06348304939270019 , lr: 0.01990545902237324\n",
            "epoch: 9600 , acc: 0.95 , loss: 0.22854200184345247 , data_loss: 0.16549928 , reg_loss: 0.06304271697998047 , lr: 0.019904468503417844\n",
            "epoch: 9700 , acc: 0.95 , loss: 0.22794793725013734 , data_loss: 0.1653134 , reg_loss: 0.0626345443725586 , lr: 0.019903478083036316\n",
            "epoch: 9800 , acc: 0.95 , loss: 0.22738295781612394 , data_loss: 0.16512989 , reg_loss: 0.062253072738647455 , lr: 0.019902487761213932\n",
            "epoch: 9900 , acc: 0.95 , loss: 0.22683088779449462 , data_loss: 0.16494405 , reg_loss: 0.061886835098266604 , lr: 0.019901497537935988\n",
            "epoch: 10000 , acc: 0.95 , loss: 0.2262499715089798 , data_loss: 0.16471894 , reg_loss: 0.061531030654907226 , lr: 0.019900507413187767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U4GrneF9cp0",
        "outputId": "60cce84f-0412-4b6d-bac8-086f076bec4d"
      },
      "source": [
        "# Validate the model\n",
        "# Create test dataset\n",
        "X_test, y_test = spiral_data( samples = 100 , classes = 3 )\n",
        "\n",
        "# Perform a forward pass of our testing data through this layer\n",
        "dense1.forward(X_test)\n",
        "\n",
        "# Perform a forward pass through activation function\n",
        "# takes the output of first dense layer here\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "# Perform a forward pass through second Dense layer\n",
        "# takes outputs of activation function of first layer as inputs\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "# Perform a forward pass through the activation/loss function\n",
        "# takes the output of second dense layer here and returns loss\n",
        "loss = loss_activation.forward(dense2.output, y_test)\n",
        "\n",
        "# Calculate accuracy from output of activation2 and targets\n",
        "# calculate values along first axis\n",
        "predictions = np.argmax(loss_activation.output, axis = 1 )\n",
        "\n",
        "if len (y_test.shape) == 2 :\n",
        "  y_test = np.argmax(y_test, axis = 1 )\n",
        "\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "\n",
        "print('validation, acc: ' + str(accuracy) +  ', loss: ' + str(loss) )"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation, acc: 0.8666666666666667, loss: 0.4676769\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiefU-IP9pHT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}